{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find the most frequent words by Washington\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "\n",
      "wash_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/washington_text')\n",
      "txtfiles = os.listdir(wash_text_path)\n",
      "\n",
      "washfd = FreqDist()\n",
      "\n",
      "#loop through all collected text files, and put all words in FreqDist\n",
      "for fname in txtfiles:\n",
      "    f = open(wash_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    for w in fwds:\n",
      "        if w.isalpha():\n",
      "            washfd.update([w.lower()])\n",
      "    f.close()\n",
      "\n",
      "#finally, remove all stopwords from the FreqDist (english and french)\n",
      "for w in stopwords.words('english'):\n",
      "    if w in washfd.keys():\n",
      "        washfd[w] = 0\n",
      "        \n",
      "#for w in stopwords.words('french'):\n",
      "#    if w in washfd.keys():\n",
      "#        washfd[w] = 0\n",
      "\n",
      "print washfd.most_common(50)\n",
      "#go through all the files and create a big dictionary of words.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', 5495), ('mr', 5117), ('shall', 4067), ('upon', 3617), ('sir', 3199), ('one', 3026), ('would', 2557), ('men', 2525), ('general', 2428), ('time', 2426), ('great', 2378), ('much', 2317), ('must', 2262), ('washington', 2111), ('two', 2108), ('every', 2029), ('think', 1966), ('sent', 1954), ('ditto', 1867), ('good', 1863), ('last', 1790), ('made', 1776), ('letter', 1742), ('officers', 1724), ('make', 1700), ('give', 1674), ('yr', 1674), ('send', 1671), ('take', 1662), ('hope', 1567), ('fort', 1551), ('new', 1510), ('servt', 1493), ('without', 1432), ('best', 1431), ('order', 1430), ('john', 1395), ('well', 1368), ('know', 1358), ('woud', 1304), ('pr', 1299), ('soon', 1294), ('first', 1286), ('pay', 1274), ('part', 1257), ('many', 1221), ('also', 1205), ('orders', 1186), ('day', 1186), ('could', 1178)]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#In this cell, extract names of French letters written by John Adams. Need to be eliminated from analysis.\n",
      "\n",
      "f = open('adams_french_letters', 'r')\n",
      "skip = True\n",
      "frenchfnames = []\n",
      "for line in f:\n",
      "    #skips the first line in the document\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    #each additional line is the name of a file.\n",
      "    frenchfnames += [line[:-1]]\n",
      "f.close()\n",
      "\n",
      "#check that \"frenchfnames\" has the correct length (should be 705); print out the first few.\n",
      "print \"number of french letters found: \",\n",
      "print len(frenchfnames)\n",
      "print frenchfnames[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of french letters found:  705\n",
        "['adams_1192', 'adams_1352', 'adams_3575', 'adams_3942', 'adams_1732', 'adams_3678', 'adams_3909', 'adams_1500', 'adams_4700', 'adams_3556']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### IN NLTK 3.0, \"FREQDIST.INC\" WAS REPLACED WITH \"FREQDIST.UPDATE\" ###\n",
      "### IN THE NEW NLTK, \"FD.ITEMS()\" DOESN'T RETURN SORTED LIST - FOR THAT, USE \"FD.MOST_COMMON(N)\" ###\n",
      "\n",
      "#Find words most frequently used by Adams\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "\n",
      "adams_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')\n",
      "txtfiles = os.listdir(adams_text_path)\n",
      "\n",
      "adamsfd = FreqDist()\n",
      "\n",
      "##### TAKE OUT ALL FRENCH FILES #### [need above cell to run...]\n",
      "for fname in txtfiles:\n",
      "    if fname in frenchfnames:\n",
      "        txtfiles.remove(fname)\n",
      "\n",
      "#loop through all collected text files, and put all words in FreqDist\n",
      "for fname in txtfiles:\n",
      "    f = open(adams_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    for w in fwds:\n",
      "        if w.isalpha():\n",
      "            adamsfd.update([w.lower()])\n",
      "    f.close()\n",
      "\n",
      "#finally, remove all stopwords from the FreqDist (english and french)\n",
      "for w in stopwords.words('english'):\n",
      "    if w in adamsfd.keys():\n",
      "        adamsfd[w] = 0\n",
      "        \n",
      "for w in stopwords.words('french'):\n",
      "    if w in adamsfd.keys():\n",
      "        adamsfd[w] = 0\n",
      "\n",
      "# [this is no longer a sorted list]\n",
      "print adamsfd.most_common(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('fawn', 1), ('carressd', 1), ('saywhy', 1), ('woods', 11), ('spiders', 1), ('hanging', 13), ('woody', 1), ('hastily', 18), ('loveryssel', 1), ('regularize', 1), ('disobeying', 1), ('mutinied', 1), ('adviced', 2), ('scold', 2), ('scoly', 1), ('refunding', 2), ('mutinies', 1), ('alphabetic', 1), ('discribed', 1), ('senceable', 1), ('grenadiers', 1), ('stipulate', 12), ('stipulatd', 3), ('appropriation', 2), ('fullblooded', 1), ('bringing', 71), ('discribes', 1), ('advices', 50), ('liaisons', 2), ('toeleg', 1), ('wooden', 2), ('wednesday', 53), ('abilitys', 2), ('probabil', 1), ('immunities', 21), ('unlukily', 1), ('insular', 5), ('mislkte', 1), ('ziektens', 2), ('complainers', 1), ('feasibility', 1), ('bannister', 1), ('sooth', 5), ('trumpetts', 1), ('sustaining', 2), ('consenting', 12), ('landsagainst', 1), ('errors', 34), ('cooking', 1), ('familles', 1)]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#doublecheck what is in adamsfd.\n",
      "print adamsfd.most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', 6588), ('would', 6476), ('shall', 5611), ('upon', 5292), ('great', 4972), ('one', 4449), ('sir', 3991), ('much', 3945), ('every', 3254), ('congress', 3145)]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#figure out how to extract sentiment information from sentiwordnet\n",
      "import nltk\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "\n",
      "#the main problem is that words can be used in many different senses. We have to figure out what sense the word\n",
      "#  is used in, and then from that, we can infer the sentimentality of the word.\n",
      "#  For now, it may be good to just take an average of all the senses."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Custom sentimentality metric: take the most common 1000 words used by Adams, then weight their sentimentality\n",
      "#  by their frequency. Then find the top 200 of the resulting list.\n",
      "import nltk\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "\n",
      "#find the words that Adams uses most frequently and possibly change their format.\n",
      "adams_1000 = adamsfd.most_common(1000)\n",
      "\n",
      "#multiply the frequency of these words by their sentiment index from sentiwordnet.\n",
      "adams_pos = []\n",
      "adams_neg = []\n",
      "\n",
      "for wf in adams_1000:\n",
      "    word = wf[0]\n",
      "    freq = wf[1]\n",
      "    #first, determine if this word exists in Sentiwordnet\n",
      "    ssets = swn.senti_synsets(word)\n",
      "    if len(ssets) == 0:\n",
      "        #doesn't deal with misspelled words: may want to correct minor misspellings later.\n",
      "        continue\n",
      "        \n",
      "    #for each word in sentiwordnet and \"adams_1000\", find its average positivity and negativity.\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for s in ssets:\n",
      "        pos += s.pos_score()\n",
      "        neg += s.neg_score()\n",
      "    \n",
      "    #the average we are taking doesn't take into account the frequency with which each type of word\n",
      "    #   is used. This may be a next step.\n",
      "    if pos > 0.01:\n",
      "        posweight = freq * pos / len(ssets)\n",
      "        adams_pos += [(word, posweight)]\n",
      "    if neg > 0.01:\n",
      "        negweight = freq * neg / len(ssets)\n",
      "        adams_neg += [(word, negweight)]\n",
      "\n",
      "        \n",
      "#now, we should have two lists of words that have a positive or negative connotation in sentiwordnet,\n",
      "#   plus their weighted scores. The next step is to sort these words by score and print results.\n",
      "adams_pos.sort(key = lambda tup: tup[1], reverse=True)\n",
      "adams_neg.sort(key = lambda tup: tup[1], reverse=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Test out the most sentimental words found for John Adams\n",
      "import sys\n",
      "print adams_pos[:10]\n",
      "print adams_neg[:10]\n",
      "\n",
      "\n",
      "#now, write Adams' most sentimental words into a file.\n",
      "f = open('adams_senti_wds', 'w')\n",
      "f.write('Positive:\\n')\n",
      "for i in range(200):\n",
      "    word = adams_pos[i][0]\n",
      "    score = adams_pos[i][1]\n",
      "    f.write(str(i) + '. ' + word + '\\t' + \"%.3f\\n\" % score)\n",
      "    \n",
      "f.write('=====================\\nNegative:\\n')\n",
      "for i in range(len(adams_neg)):\n",
      "    word = adams_neg[i][0]\n",
      "    score = adams_neg[i][1]\n",
      "    f.write(str(i) + '. ' + word + '\\t' + '%.3f\\n' % score)\n",
      "    \n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('much', 634.0178571428571), ('honour', 469.92857142857144), ('affairs', 117.6), ('want', 107.25), ('absolutely', 85.0), ('absolute', 42.25), ('fit', 42.0), ('circumstances', 38.25), ('nature', 36.4), ('master', 24.75)]\n",
        "[('much', 140.89285714285714), ('want', 107.25), ('wrong', 49.785714285714285), ('affairs', 44.1), ('disgrace', 36.0), ('former', 24.3125), ('pretence', 24.0), ('absolute', 21.125), ('tyranny', 21.0), ('troubled', 20.3125)]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Custom sentimentality metric: take the most common 1000 words used by Washington, then weight their sentimentality\n",
      "#  by their frequency. Then find the top 200 of the resulting list.\n",
      "import nltk\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "\n",
      "#find the words that Adams uses most frequently and possibly change their format.\n",
      "wash_1000 = washfd.most_common(1000)\n",
      "\n",
      "#multiply the frequency of these words by their sentiment index from sentiwordnet.\n",
      "wash_pos = []\n",
      "wash_neg = []\n",
      "\n",
      "for wf in wash_1000:\n",
      "    word = wf[0]\n",
      "    freq = wf[1]\n",
      "    #first, determine if this word exists in Sentiwordnet\n",
      "    ssets = swn.senti_synsets(word)\n",
      "    if len(ssets) == 0:\n",
      "        #doesn't deal with misspelled words: may want to correct minor misspellings later.\n",
      "        continue\n",
      "        \n",
      "    #for each word in sentiwordnet and \"adams_1000\", find its average positivity and negativity.\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for s in ssets:\n",
      "        pos += s.pos_score()\n",
      "        neg += s.neg_score()\n",
      "    \n",
      "    #the average we are taking doesn't take into account the frequency with which each type of word\n",
      "    #   is used. This may be a next step.\n",
      "    if pos > 0.01:\n",
      "        posweight = freq * pos / len(ssets)\n",
      "        wash_pos += [(word, posweight)]\n",
      "    if neg > 0.01:\n",
      "        negweight = freq * neg / len(ssets)\n",
      "        wash_neg += [(word, negweight)]\n",
      "\n",
      "        \n",
      "#now, we should have two lists of words that have a positive or negative connotation in sentiwordnet,\n",
      "#   plus their weighted scores. The next step is to sort these words by score and print results.\n",
      "wash_pos.sort(key = lambda tup: tup[1], reverse=True)\n",
      "wash_neg.sort(key = lambda tup: tup[1], reverse=True)\n",
      "\n",
      "#print out the best words\n",
      "print wash_pos[:20]\n",
      "print \"\"\n",
      "print wash_neg[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('good', 960.609375), ('proper', 756.75), ('best', 593.6783478260869), ('well', 447.4728), ('might', 376.125), ('much', 372.375), ('new', 314.5833333333333), ('necessary', 311.25), ('hope', 304.69444444444446), ('better', 299.4732222222222), ('dear', 284.0), ('must', 282.75), ('think', 263.30357142857144), ('respect', 241.66666666666666), ('know', 240.47916666666666), ('happy', 226.625), ('agreeable', 210.0), ('time', 202.16666666666666), ('goods', 186.5), ('esteem', 186.3)]\n",
        "\n",
        "[('must', 659.75), ('might', 501.5), ('little', 467.075), ('humble', 398.5), ('sufficient', 348.75), ('never', 304.0625), ('sorry', 198.125), ('hope', 195.875), ('mutilated', 184.5), ('glad', 184.375), ('yet', 177.91666666666666), ('soon', 161.75), ('fear', 154.0), ('bad', 145.95), ('militia', 143.75), ('different', 135.3125), ('trouble', 128.36363636363637), ('want', 118.09722222222223), ('immediately', 112.125), ('unhappy', 104.0)]\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Print out the most sentimental words found for Washington\n",
      "\n",
      "#now, write Washington's most sentimental words into a file.\n",
      "f = open('wash_senti_wds', 'w')\n",
      "f.write('Positive:\\n')\n",
      "for i in range(200):\n",
      "    word = wash_pos[i][0]\n",
      "    score = wash_pos[i][1]\n",
      "    f.write(str(i) + '. ' + word + '\\t' + \"%.3f\\n\" % score)\n",
      "    \n",
      "f.write('=====================\\nNegative:\\n')\n",
      "for i in range(200):\n",
      "    word = wash_neg[i][0]\n",
      "    score = wash_neg[i][1]\n",
      "    f.write(str(i) + '. ' + word + '\\t' + '%.3f\\n' % score)\n",
      "    \n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#condense the above two files into a single file\n",
      "\n",
      "#extract info from files created above first\n",
      "f = open('adams_senti_wds')\n",
      "ftxt = f.read()\n",
      "[fpos, fneg] = ftxt.split('=====================\\n')\n",
      "lpos = fpos.split('\\n')\n",
      "lneg = fneg.split('\\n')\n",
      "\n",
      "#populate list of positive tuples\n",
      "adams_poswds = []\n",
      "skip = True\n",
      "for l in lpos:\n",
      "    #skip the first line, which just explains things.\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    larr = l.split()\n",
      "    #if the line is empty, skip.\n",
      "    if len(larr) == 0:\n",
      "        continue\n",
      "    word = larr[1]\n",
      "    score = float(larr[2])\n",
      "    adams_poswds += [(word, score)]\n",
      "    \n",
      "#populate list of negative tuples\n",
      "adams_negwds = []\n",
      "skip = True\n",
      "for l in lneg:\n",
      "    #skip the first line, which just explains things.\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    larr = l.split()\n",
      "    #if the line is empty, skip.\n",
      "    if len(larr) == 0:\n",
      "        continue\n",
      "    word = larr[1]\n",
      "    score = float(larr[2])\n",
      "    adams_negwds += [(word, score)]\n",
      "f.close()\n",
      "    \n",
      "#now extract info from washington's files\n",
      "f = open('wash_senti_wds')\n",
      "ftxt = f.read()\n",
      "[fpos, fneg] = ftxt.split('=====================\\n')\n",
      "lpos = fpos.split('\\n')\n",
      "lneg = fneg.split('\\n')\n",
      "\n",
      "#populate list of positive tuples\n",
      "wash_poswds = []\n",
      "skip = True\n",
      "for l in lpos:\n",
      "    #skip the first line, which just explains things.\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    larr = l.split()\n",
      "    #if the line is empty, skip.\n",
      "    if len(larr) == 0:\n",
      "        continue\n",
      "    word = larr[1]\n",
      "    score = float(larr[2])\n",
      "    wash_poswds += [(word, score)]\n",
      "    \n",
      "#populate list of negative tuples\n",
      "wash_negwds = []\n",
      "skip = True\n",
      "for l in lneg:\n",
      "    #skip the first line, which just explains things.\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    larr = l.split()\n",
      "    #if the line is empty, skip.\n",
      "    if len(larr) == 0:\n",
      "        continue\n",
      "    word = larr[1]\n",
      "    score = float(larr[2])\n",
      "    wash_negwds += [(word, score)]\n",
      "f.close()\n",
      "\n",
      "#now, write all the above information into a single file.\n",
      "fout = open('senti_wordfreq_analysis', 'w')\n",
      "fout.write('Analysis for Positive Words:\\n\\n')\n",
      "fout.write(\"+--------------------+-----------------------------+\\n\")\n",
      "fout.write(\"  Adams Word         |       Washington Word\\n\")\n",
      "fout.write(\"+--------------------+-----------------------------+\\n\")\n",
      "\n",
      "#start with positive information\n",
      "for i in range(200):\n",
      "    aword = adams_poswds[i][0]\n",
      "    ascore = adams_poswds[i][1]\n",
      "    wword = wash_poswds[i][0]\n",
      "    wscore = wash_poswds[i][1]\n",
      "    fout.write(str(i) + '. ' + aword + ': ' + \"%.3f\\t\" % ascore + '\\t' + wword + ': ' + \"%.3f\\n\" % wscore + '\\n')\n",
      "\n",
      "#repeat above for all the negative words\n",
      "fout.write('\\n\\n Analysis for Negative Words:\\n\\n')\n",
      "fout.write(\"+--------------------+-----------------------------+\\n\")\n",
      "fout.write(\"  Adams Word         |       Washington Word\\n\")\n",
      "fout.write(\"+--------------------+-----------------------------+\\n\")    \n",
      "for i in range(len(adams_negwds)):\n",
      "    aword = adams_negwds[i][0]\n",
      "    ascore = adams_negwds[i][1]\n",
      "    wword = wash_negwds[i][0]\n",
      "    wscore = wash_negwds[i][1]\n",
      "    fout.write(str(i) + '. ' + aword + ': ' + \"%.3f\\t\" % ascore + '\\t' + wword + ': ' + \"%.3f\\n\" % wscore + '\\n')\n",
      "\n",
      "\n",
      "\n",
      "fout.close()\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}