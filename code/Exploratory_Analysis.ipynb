{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#In this file, I will start unpacking the data I have crawled, extracting text from the HTMLs, and doing simple\n",
      "# analysis on what kinds of words are most frequent in these files.\n",
      "\n",
      "import os\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "#current location is sr_iw/code. Need to get to sr_iw/data/crawling/adams\n",
      "adams_data_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams')\n",
      "adams_op_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')\n",
      "\n",
      "\n",
      "#start by experimenting on adams_1\n",
      "f = open(adams_data_path + '/adams_3193')\n",
      "adams1txt = f.read()\n",
      "adams1soup = BeautifulSoup(adams1txt, 'html.parser')\n",
      "\n",
      "print adams1soup.find(\"div\", {\"class\": \"docbody\"}).get_text()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Sir\n",
        "Brussels March 21. 1781\n",
        "\n",
        "\n",
        "\n",
        "The Capture of St. Eustache, which was to be expected, and the immense Acquisition of Property in the Ships taken will surely rouse the antient Spirit of the Dutch, which was always greatest in Times of the greatest Calamity. Or will this Misfortune be wickedly turned against the Friends of the Independance of the States? I am anxious to hear how this News will be recievd in Holland. Your Excellency Observes, that the Inhabitants of St. Eustache were lulled into Se-[223]curity by the reports of Captn. Byland.1 His Instructions surely must have been very particular to have made Him act in that manner. Your Excellency sees too the Malicious pleasure, that Rodney takes in finding the blow will be most felt by Amsterdam. The internal and external Ennemies of Holland act wonderfully in Concert. The blow is certainly a great one, and will be felt by the French and Americans too, but it will be felt by the English likewise, who had a considerable Property in the Island and Ships. It ought surely rouse the Dutch and all Powers to the Utmost. I know not the State of the Preparations where you are, but if the Dutch had ten ships of the Line ready, they ought to be sent to the Downs, which they would soon clear of the Ennemy. They might then lay a fortnight at the Mouth of the Thames, and wait until they are joined by 10 Ships more, and sail together through the Channel and join the Ships at Brest, and thereby make a considerable Squadron to wait the return of the English fleet from Gibraltar. Should the English meet with the Spanyards they may be beat, the Ships will certainly be much shattered, and thereby become an Easy prey to a fresh force at the mouth of the Channel.\n",
        "How, Sir, will Russia Act on this Event? Will not the Empress take a decided Part? When She insists, that the Independancy should be the basis of Her Mediation, She seems to have laid the foundation of the Conduct now to be held, if it cannot be had by her mediation, it ought to be so be her Acting publickly and Hostilely. The Times require that the insultd Powers should have one Object or Else they will continue to Act without Vigor and without concert. So long as they do so, England is warranted in her pursuit of what otherwise appears a mad and desperate Game. She sees, that the great force against Her is not used with Spirit and Judgment. She sees that Each State in Enmity to Her has private and different Views; and therefore the formidable Combination may be baffled and defeated, but if it would have the same one Object in View, An Object which will assure to all, what all wish to Attain, the Independancy of America, it would have a sure and certain rule for the Directions of its Operations. Until that is done, England will persist and may triumph Altho to her Ruin.\n",
        "It is plain, Holland must submit or Else Act with Vigor, and what an opportunity has She at this moment of making a decisive Stroke, if she was prepared and had not a Mill Stone about Her neck! The whole of the English force is supposed to have left the Kingdom, and thereby Her Coasts are exposed to depredations and Attacks. Should the English Fleet be beat by the Spanish or french Squadrons, England [224]must be ruined at Home whilst She rejoices at her foreign Successes, but she depends on her secret Friends, which embarrass the operations of the Dutch. The taking of St. Eustache will I think produce some great Event one way or other. I wait with Anxiety to hear it. Will Russia Sir, amuse Herself with the Idea of a Treaty? How could She suffer Johnstones Squadron to sail evidently with the intention to Attack the Dutch in the East? Does she not see that England means not to come into the Terms proposed? I wish the United States had a Man in a public Character at Petersburgh, Could not Mr. Dana do much good there? His manner and his Knowledge would draw the Attention of that Court. The Conduct of England is such, as to alarm all and promises a fair reception to any one Acting for the general Interest of Europe. Why should not the Northern Powers adopt at least and at last my favorite Idea, and lay an Embargo on all English Ships now in the Sound,2 and prevent others entering in. This alone would finish the War as it would have Stopped its Continuance long since. There is something in Politics beyond a Plain Mans Comprehension.\n",
        "\n",
        "I am with the greatest Respect Sir Your Excellencys Most faithful & Obedient Humble Servt.\n",
        "Edm: Jenings\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#perform the procedure from above on all raw html files, and store the results in new text files.\n",
      "# NEVER NEED TO RUN THIS AGAIN - IT WAS ONLY FOR FILTERING OUT INITIAL NOISE (FOR ADAMS)\n",
      "\n",
      "import os\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "#current location is sr_iw/code. Need to get to sr_iw/data/crawling/adams\n",
      "adams_data_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams')\n",
      "adams_op_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')\n",
      "\n",
      "#5181 is the number of raw html files in the directory \"adams\".\n",
      "for i in range(5181):\n",
      "    #check if the given file exists in the target directory\n",
      "    if os.path.isfile(adams_op_path + '/adams_' + str(i)):\n",
      "        continue\n",
      "    \n",
      "    #if the given file does not exist in the target directory, open the html file and convert text to bfsoup.\n",
      "    f = open(adams_data_path + '/adams_' + str(i))\n",
      "    adamstxt = f.read()\n",
      "    adamssoup = BeautifulSoup(adamstxt, 'html.parser')\n",
      "    \n",
      "    #for all the given files, extract the raw text from the file. If none can be extracted, just print [[none]]\n",
      "    try:\n",
      "        text = adamssoup.find(\"div\", {\"class\": \"docbody\"}).get_text()\n",
      "    except:\n",
      "        text = \"[[none]]\"\n",
      "    f.close()\n",
      "    \n",
      "    #put the extracted text in a new file in the new directory.\n",
      "    fout = open(adams_op_path + '/adams_' + str(i), 'w')\n",
      "    fout.write(text)\n",
      "    fout.close()\n",
      "\n",
      "    \n",
      "\n",
      "#for the future: note that several files are not letters. Some files are drafts of letters, some files contain\n",
      "#  barely any text, some files are detailed inventories, some files are written in french -_____-.\n",
      "\n",
      "#have to find and weed out these files.\n",
      "#can start by getting rid of files with fewer than 100 words\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#want to try to eliminate files that are not letters \n",
      "#RUN THIS IMMEDIATELY!\n",
      "\n",
      "import os\n",
      "\n",
      "adams_rawdata_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams')\n",
      "adams_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')\n",
      "\n",
      "#start by finding files that have fewer than 100 words\n",
      "txtfiles = os.listdir(adams_text_path)\n",
      "\n",
      "fout = open('lessthan_100', 'w')\n",
      "lessthan_100 = 0\n",
      "for fname in txtfiles:\n",
      "    f = open(adams_text_path + '/' + fname)\n",
      "    ftxt = f.read()\n",
      "    f.close()\n",
      "    if len(ftxt.split()) < 100:\n",
      "        fout.write(fname)\n",
      "        lessthan_100 += 1\n",
      "\n",
      "fout.close()        \n",
      "print lessthan_100\n",
      "\n",
      "\n",
      "#This heuristic ( < 100) is pretty good at finding useless documents. However, it's not perfect. There are some\n",
      "# sentiment filled letters with less than 100 words, and there may be some useless letters that are less than\n",
      "# 100 words.\n",
      "\n",
      "#next: collect a list of these documents with less than 100 words.\n",
      "#also, do some analysis of the words used by JA, vs. GW!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "448\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#want to eliminate files that are written in french\n",
      "#NEVER NEED TO RUN THIS CELL AGAIN - NAMES SHOULD BE PRINTED INTO A FILE.\n",
      "\n",
      "#want to start by experimenting...\n",
      "#french letter = 3333\n",
      "#english letter = 88\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "\n",
      "adams_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')\n",
      "txtfiles = os.listdir(adams_text_path)\n",
      "\n",
      "#experimenting\n",
      "\"\"\"\n",
      "f = open(adams_text_path + '/adams_3333')\n",
      "frlet = f.read().split()\n",
      "frenchfd = FreqDist(frlet)\n",
      "frenchsw = 0\n",
      "engsw = 0\n",
      "for w in frenchfd.keys():\n",
      "    if w in stopwords.words('french'):\n",
      "        frenchsw += frenchfd[w]\n",
      "    elif w in stopwords.words('english'):\n",
      "        engsw += frenchfd[w]\n",
      "print \"french sw: \" + str(frenchsw) + \";   eng sw: \" + str(engsw)\n",
      "\n",
      "\n",
      "f1 = open(adams_text_path + '/adams_88')\n",
      "englet = f1.read().split()\n",
      "engfd = FreqDist(englet)\n",
      "frenchsw = 0\n",
      "engsw = 0\n",
      "for w in engfd.keys():\n",
      "    if w in stopwords.words('french'):\n",
      "        frenchsw += engfd[w]\n",
      "    elif w in stopwords.words('english'):\n",
      "        engsw += engfd[w]\n",
      "print \"french sw: \" + str(frenchsw) + \";   eng sw: \" + str(engsw)\n",
      "\"\"\"\n",
      "\n",
      "frenchfnames = []\n",
      "for fname in txtfiles:\n",
      "    f = open(adams_text_path + '/' + fname)\n",
      "    fd = FreqDist(f.read().split())\n",
      "    frenchsw = 0\n",
      "    engsw = 0\n",
      "    for w in fd.keys():        \n",
      "        if w in stopwords.words('french'):\n",
      "            frenchsw += fd[w]\n",
      "        elif w in stopwords.words('english'):\n",
      "            engsw += fd[w]\n",
      "    \n",
      "    if frenchsw > (engsw + 20):\n",
      "        frenchfnames += [fname]\n",
      "\n",
      "print frenchfnames[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['adams_1192', 'adams_1352', 'adams_3575', 'adams_3942', 'adams_1732', 'adams_3678', 'adams_3909', 'adams_1500', 'adams_4700', 'adams_3556']\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(frenchfnames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "705\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Print the French Filenames into a file. \n",
      "# NEVER NEED TO RUN THIS AGAIN.\n",
      "\n",
      "#this file contains the names of the files in the 'data/crawling/adams/' directory that are written\n",
      "#  in French\n",
      "#  caveat: it may not be exhaustive, and other documents may be a mix of french and english.\n",
      "f = open('adams_french_letters', 'w')\n",
      "f.write('This file contains the names of the files in the \"data/crawling/adams/\" directory written in French.\\n')\n",
      "for docname in frenchfnames:\n",
      "    f.write(docname + '\\n')\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#In this cell, extract names of French letters written by John Adams.\n",
      "\n",
      "f = open('adams_french_letters', 'r')\n",
      "skip = True\n",
      "frenchfnames = []\n",
      "for line in f:\n",
      "    #skips the first line in the document\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    #each additional line is the name of a file.\n",
      "    frenchfnames += [line[:-1]]\n",
      "f.close()\n",
      "\n",
      "#check that \"frenchfnames\" has the correct length (should be 705); print out the first few.\n",
      "print \"number of french letters found: \",\n",
      "print len(frenchfnames)\n",
      "print frenchfnames[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of french letters found:  705\n",
        "['adams_1192', 'adams_1352', 'adams_3575', 'adams_3942', 'adams_1732', 'adams_3678', 'adams_3909', 'adams_1500', 'adams_4700', 'adams_3556']\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now, want to look at words used most commonly by adams.\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "\n",
      "adams_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')\n",
      "txtfiles = os.listdir(adams_text_path)\n",
      "\n",
      "adamsfd = FreqDist()\n",
      "\n",
      "##### TAKE OUT ALL FRENCH FILES #### [need above cell to run...]\n",
      "for fname in txtfiles:\n",
      "    if fname in frenchfnames:\n",
      "        txtfiles.remove(fname)\n",
      "\n",
      "#loop through all collected text files, and put all words in FreqDist\n",
      "for fname in txtfiles:\n",
      "    f = open(adams_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    for w in fwds:\n",
      "        if w.isalpha():\n",
      "            adamsfd.inc(w.lower())\n",
      "    f.close()\n",
      "\n",
      "#finally, remove all stopwords from the FreqDist (english and french)\n",
      "for w in stopwords.words('english'):\n",
      "    if w in adamsfd.keys():\n",
      "        adamsfd[w] = 0\n",
      "        \n",
      "for w in stopwords.words('french'):\n",
      "    if w in adamsfd.keys():\n",
      "        adamsfd[w] = 0\n",
      "\n",
      "#print a list of the 50 most commonly used words\n",
      "print adamsfd.most_common(50)\n",
      "# print total unique words used.\n",
      "print ''\n",
      "print 'Total number of unique words used: ',\n",
      "print len(adamsfd)\n",
      "\n",
      "#now, find the total number of words (not necessarily unique) and print them out\n",
      "total_wds = 0\n",
      "for w in adamsfd:\n",
      "    total_wds += adamsfd[w]\n",
      "print ''\n",
      "print 'Total word count: ' + str(total_wds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', 6588), ('would', 6476), ('shall', 5611), ('upon', 5292), ('great', 4972), ('one', 4449), ('sir', 3991), ('much', 3945), ('every', 3254), ('congress', 3145), ('must', 3119), ('letter', 3060), ('time', 2657), ('think', 2638), ('make', 2636), ('adams', 2600), ('made', 2588), ('two', 2500), ('states', 2465), ('us', 2415), ('good', 2362), ('general', 2352), ('without', 2350), ('well', 2349), ('united', 2284), ('new', 2273), ('give', 2205), ('could', 2186), ('said', 2134), ('know', 2098), ('last', 2064), ('america', 2028), ('many', 2028), ('take', 2013), ('first', 1913), ('american', 1905), ('hope', 1881), ('john', 1861), ('never', 1807), ('people', 1799), ('dear', 1781), ('yet', 1764), ('present', 1762), ('part', 1726), ('see', 1701), ('might', 1688), ('wish', 1630), ('letters', 1603), ('humble', 1600), ('cannot', 1550)]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## store adams' most common words in a single file\n",
      "# ONLY NEED TO RUN THIS ONCE.\n",
      "\n",
      "f = open('adams_wds_nostop', 'w')\n",
      "rank = 0\n",
      "for w in adamsfd.items():\n",
      "    f.write(w[0] + ' ' + str(rank) + '\\n')\n",
      "    rank += 1\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### FIND WORDS MOST FREQUENTLY USED BY WASHINGTON (FOR COMPARISON ANALYSIS)\n",
      "\n",
      "#now, want to look at words used most commonly by washington.\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "\n",
      "wash_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/washington_text')\n",
      "txtfiles = os.listdir(wash_text_path)\n",
      "\n",
      "washfd = FreqDist()\n",
      "\n",
      "#loop through all collected text files, and put all words in FreqDist\n",
      "for fname in txtfiles:\n",
      "    f = open(wash_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    for w in fwds:\n",
      "        if w.isalpha():\n",
      "            washfd.inc(w.lower())\n",
      "    f.close()\n",
      "\n",
      "#finally, remove all stopwords from the FreqDist (english and french)\n",
      "for w in stopwords.words('english'):\n",
      "    if w in washfd.keys():\n",
      "        washfd[w] = 0\n",
      "        \n",
      "for w in stopwords.words('french'):\n",
      "    if w in washfd.keys():\n",
      "        washfd[w] = 0\n",
      "\n",
      "print washfd.items()[:50]\n",
      "#go through all the files and create a big dictionary of words.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-50a4f9da5469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'french'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwashfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mwashfd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/nltk/probability.pyc\u001b[0m in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \"\"\"\n\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_keys_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for each word (w) in the Wash fd, find the freq(w, Wash) / [freq(w, Adams) + 5] (i.e. relfq)\n",
      "# this adjusts for words not being used as frequently in either distribution, and just finds words that \n",
      "#   Washington was much more likely to use than Adams.\n",
      "\n",
      "wash_words_relfq = []\n",
      "akeys = adamsfd.keys()\n",
      "\n",
      "for w in washfd.keys():\n",
      "    wfreq = washfd[w]\n",
      "    \n",
      "    #now find afreq\n",
      "    afreq = 0\n",
      "    if w in akeys:\n",
      "        afreq = adamsfd[w]\n",
      "    \n",
      "    #1. add five to reduce noise; 2. want result to be a float...\n",
      "    wfval = float(wfreq)*10000 / (afreq+5)\n",
      "    wfval = int(wfval) / float(10000)\n",
      "    wash_words_relfq += [(wfval, w)]\n",
      "\n",
      "wash_words_relfq.sort(reverse=True)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print wash_words_relfq[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(113.4, 'dozn'), (96.2727, 'acct'), (93.4, 'fairfax'), (93.4, 'custis'), (62.2333, 'ditto'), (61.5, 'mutilated'), (58.1666, 'tobo'), (58.0, 'winchester'), (51.8, 'countersign'), (51.3571, 'captn'), (50.2, 'majr'), (47.4, 'balle'), (46.3928, 'pr'), (45.8333, 'yds'), (39.4, 'accts'), (36.7142, 'williamsburg'), (36.1428, 'pd'), (34.2, 'carlyle'), (33.3333, 'loudoun'), (31.2, 'custiss'), (30.4285, 'abt'), (28.8, 'gist'), (27.625, 'contra'), (27.4, 'dinwiddie'), (26.0, 'illegible'), (25.8571, 'alexandria'), (25.6, 'lund'), (25.3636, 'yr'), (24.6, 'gw'), (24.2, 'virga'), (24.1428, 'stewart'), (23.2, 'valentine'), (23.0909, 'mercer'), (23.0, 'richd'), (23.0, 'dble'), (23.0, 'crawford'), (22.7272, 'doctr'), (22.0, 'augt'), (22.0, 'affecte'), (21.8, 'ensign'), (21.5714, 'accot'), (20.6666, 'parke'), (20.0, 'instt'), (19.6363, 'hhds'), (18.9655, 'wm'), (18.8888, 'creek'), (18.6666, 'cary'), (18.25, 'gentn'), (18.2, 'prct'), (18.2, 'byrd'), (18.2, 'augusta'), (18.1666, 'honr'), (18.0476, 'vernon'), (17.8, 'intt'), (17.8, 'cherokees'), (17.2, 'shoes'), (17.0, 'wmsburg'), (16.5, 'willm'), (16.5, 'cumberland'), (16.3333, 'revd'), (16.0, 'serjt'), (16.0, 'ohio'), (15.8, 'ramsay'), (15.8, 'hanbury'), (15.6, 'regimt'), (15.4, 'nails'), (15.0, 'waggons'), (14.68, 'morrow'), (14.4, 'groce'), (14.4, 'ass'), (14.3793, 'lewis'), (14.25, 'ulto'), (14.2352, 'mount'), (14.1666, 'surveys'), (14.0444, 'indians'), (14.0, 'wn'), (14.0, 'bryan'), (13.8, 'dandridge'), (13.6666, 'mercers'), (13.4285, 'shipd'), (13.4, 'bouquet'), (13.3333, 'sd'), (13.3303, 'servt'), (13.2363, 'genl'), (13.2222, 'wth'), (13.2, 'waggener'), (13.1538, 'cards'), (12.9285, 'regt'), (12.8591, 'obedt'), (12.8571, 'potomack'), (12.6, 'reas'), (12.6, 'pumps'), (12.5, 'fredericksburg'), (12.4, 'qty'), (12.4, 'detachmt'), (12.4, 'cooke'), (12.4, 'cacapon'), (12.3333, 'lieutt'), (12.2, 'steel'), (12.2, 'pattersons')]\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for each word (w) in the Adams fd, find the freq(w, Adams) / [freq(w, Wash) + 5] (i.e. relfq)\n",
      "# this adjusts for words not being used as frequently in either distribution, and just finds words that \n",
      "#   Adams was much more likely to use than Washington.\n",
      "\n",
      "adams_words_relfq = []\n",
      "wkeys = washfd.keys()\n",
      "\n",
      "for w in adamsfd.keys():\n",
      "    afreq = adamsfd[w]\n",
      "    \n",
      "    #now find afreq\n",
      "    wfreq = 0\n",
      "    if w in wkeys:\n",
      "        wfreq = adamsfd[w]\n",
      "    \n",
      "    #1. add five to reduce noise; 2. want result to be a float...\n",
      "    afval = float(afreq)*10000 / (wfreq+5)\n",
      "    afval = int(afval) / float(10000)\n",
      "    adams_words_relfq += [(afval, w)]\n",
      "\n",
      "adams_words_relfq.sort(reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#before this yields any useful data, I have to eliminate french letters.\n",
      "print adams_words_relfq[:200]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(64.6, 'hague'), (52.8, 'laurens'), (41.4, 'vergennes'), (37.2, 'mightinesses'), (35.2, 'passy'), (34.6, 'plenipotentiary'), (33.6, 'het'), (32.4, 'thaxter'), (32.0, 'republick'), (31.4, 'emperor'), (31.0, 'nantes'), (30.2, 'jenings'), (27.6, 'neufville'), (26.4, 'thaxters'), (23.6, 'rc'), (23.2, 'jan'), (23.0, 'ja'), (22.2, 'hotel'), (21.8, 'ratification'), (21.2, 'bordeaux'), (21.0, 'britannic'), (20.6, 'mediation'), (19.8, 'belligerent'), (19.4, 'wilhem'), (19.4, 'dat'), (19.2, 'lande'), (19.0, 'lorient'), (17.8, 'livres'), (17.8, 'leyden'), (17.4, 'sovereignty'), (17.0, 'russian'), (17.0, 'confederation'), (16.6, 'provisional'), (16.4, 'etats'), (16.2, 'willink'), (15.8, 'feudal'), (15.6, 'contraband'), (15.4, 'salaries'), (15.4, 'passi'), (14.6, 'republican'), (14.6, 'pensionary'), (14.0, 'regency'), (13.8, 'netherlands'), (13.6, 'massachusettensis'), (13.4, 'auteuil'), (13.2, 'quelque'), (13.2, 'bondfield'), (13.0, 'barclay'), (12.8, 'sartine'), (12.6, 'shelburne'), (12.4, 'quam'), (12.2, 'merchandizes'), (12.0, 'sirs'), (12.0, 'fynje'), (12.0, 'bene'), (11.8, 'aussi'), (11.6, 'valois'), (11.4, 'sujets'), (11.4, 'official'), (11.2, 'guilders'), (11.2, 'gillon'), (11.0, 'op'), (11.0, 'diu'), (10.8, 'searle'), (10.8, 'rodney'), (10.6, 'und'), (10.6, 'tot'), (10.4, 'schweighauser'), (10.4, 'ik'), (10.2, 'vaisseaux'), (10.2, 'politicians'), (10.0, 'toutes'), (10.0, 'envoy'), (9.8, 'zo'), (9.8, 'u'), (9.8, 'petersbourg'), (9.8, 'brattle'), (9.6, 'richelieu'), (9.6, 'florins'), (9.6, 'depreciation'), (9.4, 'statutes'), (9.4, 'staphorst'), (9.4, 'izard'), (9.4, 'autre'), (9.2, 'zu'), (9.2, 'passports'), (9.2, 'lbc'), (8.8, 'preliminaries'), (8.8, 'highness'), (8.6, 'vaisseau'), (8.6, 'consuls'), (8.6, 'affaires'), (8.4, 'unis'), (8.4, 'thulemeier'), (8.4, 'destaing'), (8.2, 'wy'), (8.2, 'serene'), (8.2, 'aan'), (8.0, 'roi'), (8.0, 'negociations'), (8.0, 'compte'), (8.0, 'barbary'), (7.8, 'vienna'), (7.8, 'speculations'), (7.8, 'rue'), (7.8, 'refugees'), (7.8, 'om'), (7.8, 'masts'), (7.8, 'lune'), (7.8, 'friesland'), (7.8, 'commonwealth'), (7.8, 'avril'), (7.6, 'pownalls'), (7.6, 'ici'), (7.6, 'cas'), (7.6, 'bankers'), (7.4, 'luzac'), (7.4, 'lautre'), (7.4, 'credence'), (7.2, 'wisest'), (7.2, 'novanglus'), (7.2, 'lettres'), (7.2, 'formally'), (7.2, 'bourbon'), (7.2, 'acceed'), (7.0, 'politick'), (7.0, 'natives'), (7.0, 'ms'), (7.0, 'marchandises'), (7.0, 'madame'), (7.0, 'genet'), (7.0, 'fra'), (7.0, 'contractantes'), (7.0, 'charg'), (7.0, 'catholic'), (7.0, 'banker'), (7.0, 'alliances'), (7.0, 'alle'), (6.8, 'voor'), (6.8, 'vauguyon'), (6.8, 'letterbook'), (6.8, 'fizeaux'), (6.8, 'fayette'), (6.6, 'trait'), (6.6, 'sweden'), (6.6, 'statesmen'), (6.6, 'ru'), (6.6, 'politique'), (6.6, 'plenipotentiaries'), (6.6, 'idem'), (6.6, 'drake'), (6.6, 'delegate'), (6.4, 'quon'), (6.4, 'myn'), (6.4, 'laurenss'), (6.4, 'hartleys'), (6.4, 'excellenc'), (6.4, 'bruxelles'), (6.4, 'bon'), (6.4, 'berckel'), (6.4, 'beloved'), (6.4, 'avoir'), (6.2, 'stipulations'), (6.2, 'moins'), (6.2, 'luzerne'), (6.2, 'duc'), (6.2, 'danish'), (6.2, 'dan'), (6.2, 'coupons'), (6.2, 'congres'), (6.2, 'celle'), (6.0, 'zal'), (6.0, 'undertakers'), (6.0, 'swedish'), (6.0, 'ridley'), (6.0, 'ratifications'), (6.0, 'multitudes'), (6.0, 'contre'), (6.0, 'calkoen'), (6.0, 'burgomasters'), (6.0, 'audience'), (5.8, 'sil'), (5.8, 'pris'), (5.8, 'libert'), (5.8, 'lamerique'), (5.8, 'lahaie'), (5.8, 'jours'), (5.8, 'facteur'), (5.8, 'economic'), (5.8, 'berlin'), (5.8, 'abb'), (5.6, 'zealand'), (5.6, 'ostend'), (5.6, 'lon'), (5.6, 'independancy'), (5.6, 'imposts'), (5.6, 'ich'), (5.6, 'finance'), (5.6, 'chaumont'), (5.4, 'reciprocity')]\n"
       ]
      }
     ],
     "prompt_number": 71
    }
   ],
   "metadata": {}
  }
 ]
}