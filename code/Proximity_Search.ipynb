{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Goal of this file will be to replicate proximity search metric - as an alternative to sentiment-wordfreq metric.\n",
      "\n",
      "#what we need to do is find the number of times that the given word occurred within n words of \"excellent\" /\n",
      "#  \"good\" / \"proper\". \n",
      "\n",
      "# The definition in Bing Liu's tutorial is: PMI(w1, w2)  = log_2(P(w1 ^ w2)/[P(w1)P(w2)])\n",
      "#    here, I'm guessing P(w1) equals freq(w1) / total_wc.\n",
      "\n",
      "#need to build a reverse index for all words to find PMI\n",
      "\n",
      "\n",
      "\n",
      "#start by doing a frequency analysis of all datasets.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for Adams' frequency analysis, first pull out the french letters\n",
      "\n",
      "f = open('adams_french_letters', 'r')\n",
      "skip = True\n",
      "frenchfnames = []\n",
      "for line in f:\n",
      "    #skips the first line in the document\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    #each additional line is the name of a file.\n",
      "    frenchfnames += [line[:-1]]\n",
      "f.close()\n",
      "\n",
      "#check that \"frenchfnames\" has the correct length (should be 705); print out the first few.\n",
      "print \"number of french letters found: \",\n",
      "print len(frenchfnames)\n",
      "print frenchfnames[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of french letters found:  705\n",
        "['adams_1192', 'adams_1352', 'adams_3575', 'adams_3942', 'adams_1732', 'adams_3678', 'adams_3909', 'adams_1500', 'adams_4700', 'adams_3556']\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now, do a frequency analysis: create the \"adamsfd\" - the freqdist for all of Adams' letters and the words in them.\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "\n",
      "adams_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')\n",
      "txtfiles = os.listdir(adams_text_path)\n",
      "\n",
      "#this will eventually store all of Adams' words and their frequencies.\n",
      "adamsfd = FreqDist()\n",
      "\n",
      "##### TAKE OUT ALL FRENCH FILES #### [need above cell to run...]\n",
      "for fname in txtfiles:\n",
      "    if fname in frenchfnames:\n",
      "        txtfiles.remove(fname)\n",
      "\n",
      "#loop through all collected text files, and put all words in FreqDist\n",
      "for fname in txtfiles:\n",
      "    f = open(adams_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    for w in fwds:\n",
      "        if w.isalpha():\n",
      "            adamsfd.update([w.lower()])\n",
      "    f.close()\n",
      "\n",
      "#finally, remove all stopwords from the FreqDist (english and french)\n",
      "for w in stopwords.words('english'):\n",
      "    if w in adamsfd.keys():\n",
      "        adamsfd[w] = 0\n",
      "        \n",
      "for w in stopwords.words('french'):\n",
      "    if w in adamsfd.keys():\n",
      "        adamsfd[w] = 0\n",
      "\n",
      "# [print the fifty first entries in \"most common\". Note: this is a sorted list]\n",
      "print adamsfd.most_common(50)\n",
      "# print total unique words used.\n",
      "print ''\n",
      "print 'Total number of unique words used: ',\n",
      "print len(adamsfd)\n",
      "\n",
      "#now, find the total number of words (not necessarily unique) and print them out\n",
      "adams_totalwc = 0\n",
      "for w in adamsfd:\n",
      "    adams_totalwc += adamsfd[w]\n",
      "print ''\n",
      "print 'Total word count: ' + str(adams_totalwc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', 6588), ('would', 6476), ('shall', 5611), ('upon', 5292), ('great', 4972), ('one', 4449), ('sir', 3991), ('much', 3945), ('every', 3254), ('congress', 3145), ('must', 3119), ('letter', 3060), ('time', 2657), ('think', 2638), ('make', 2636), ('adams', 2600), ('made', 2588), ('two', 2500), ('states', 2465), ('us', 2415), ('good', 2362), ('general', 2352), ('without', 2350), ('well', 2349), ('united', 2284), ('new', 2273), ('give', 2205), ('could', 2186), ('said', 2134), ('know', 2098), ('last', 2064), ('america', 2028), ('many', 2028), ('take', 2013), ('first', 1913), ('american', 1905), ('hope', 1881), ('john', 1861), ('never', 1807), ('people', 1799), ('dear', 1781), ('yet', 1764), ('present', 1762), ('part', 1726), ('see', 1701), ('might', 1688), ('wish', 1630), ('letters', 1603), ('humble', 1600), ('cannot', 1550)]\n",
        "\n",
        "Total number of unique words used:  40066\n",
        "\n",
        "Total word count: 813461\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Next, find the freqdist for all of Washington's letters.\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "\n",
      "wash_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/washington_text')\n",
      "txtfiles = os.listdir(wash_text_path)\n",
      "\n",
      "washfd = FreqDist()\n",
      "\n",
      "#loop through all collected text files, and put all words in FreqDist\n",
      "for fname in txtfiles:\n",
      "    f = open(wash_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    for w in fwds:\n",
      "        if w.isalpha():\n",
      "            washfd.update([w.lower()])\n",
      "    f.close()\n",
      "\n",
      "#finally, remove all stopwords from the FreqDist (english and french)\n",
      "for w in stopwords.words('english'):\n",
      "    if w in washfd.keys():\n",
      "        washfd[w] = 0\n",
      "        \n",
      "#for w in stopwords.words('french'):\n",
      "#    if w in washfd.keys():\n",
      "#        washfd[w] = 0\n",
      "\n",
      "#print out the most frequent common\n",
      "print washfd.most_common(50)\n",
      "\n",
      "#print out the total number of unique words:\n",
      "print \"\"\n",
      "print \"Number of unique words: \",\n",
      "print len(washfd)\n",
      "\n",
      "#find the total number of words:\n",
      "total_wds = 0\n",
      "for w in washfd:\n",
      "    total_wds += washfd[w]\n",
      "print ''\n",
      "print 'Total word count: ' + str(total_wds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', 5495), ('mr', 5117), ('shall', 4067), ('upon', 3617), ('sir', 3199), ('one', 3026), ('would', 2557), ('men', 2525), ('general', 2428), ('time', 2426), ('great', 2378), ('much', 2317), ('must', 2262), ('washington', 2111), ('two', 2108), ('every', 2029), ('think', 1966), ('sent', 1954), ('ditto', 1867), ('good', 1863), ('last', 1790), ('made', 1776), ('letter', 1742), ('officers', 1724), ('make', 1700), ('give', 1674), ('yr', 1674), ('send', 1671), ('take', 1662), ('hope', 1567), ('fort', 1551), ('new', 1510), ('servt', 1493), ('without', 1432), ('best', 1431), ('order', 1430), ('john', 1395), ('well', 1368), ('know', 1358), ('woud', 1304), ('pr', 1299), ('soon', 1294), ('first', 1286), ('pay', 1274), ('part', 1257), ('many', 1221), ('also', 1205), ('orders', 1186), ('day', 1186), ('could', 1178)]\n",
        "\n",
        "Number of unique words:  39607\n",
        "\n",
        "Total word count: 813461\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Now, create an \"inverted index\" from Adams' files\n",
      "## [in retrospect, may not need the above two cells - although we do get a list of most frequent words...]\n",
      "# Inverted index: (filename/number, word index.)\n",
      "##  Alternatively, may be faster to simply do (wordlist[i-5:i+5]), where i is the index of the word.\n",
      "\n",
      "#loop through all text files, and build inverted index.\n",
      "\n",
      "#start building with just one word? (\"good\" / \"excellent\")\n",
      "\n",
      "from nltk.probability import FreqDist\n",
      "\n",
      "#this is the proximity constant\n",
      "prox = 10\n",
      "posdata = []\n",
      "\n",
      "#find the full list of text files (and take out french files, extracted above)\n",
      "txtfiles = os.listdir(adams_text_path)\n",
      "for fname in txtfiles:\n",
      "    if fname in frenchfnames:\n",
      "        txtfiles.remove(fname)\n",
      "\n",
      "#build a collection (posdata) of words that occur within 10 words of the word \"good\" in all files\n",
      "for fname in txtfiles:\n",
      "    f = open(adams_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    fwds = [w.lower() for w in fwds if w.isalpha()]\n",
      "    flen = len(fwds)\n",
      "    for wi in range(len(fwds)):\n",
      "        #if we find the word \"good\", store the whole set of words that are within distance \"prox\" of the word.\n",
      "        #we can add other words here later...\n",
      "        if fwds[wi] == \"good\":\n",
      "            posdata += fwds[max(0, wi-prox):min(flen, wi+prox)]\n",
      "    f.close()\n",
      "\n",
      "print posdata[:10]\n",
      "\n",
      "#now, create a freqdist from this collection\n",
      "adamsposfd = FreqDist(posdata)\n",
      "print ''\n",
      "print adamsposfd.most_common(20)\n",
      "print ''\n",
      "print 'total positive words: ',\n",
      "print len(adamsposfd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['made', 'would', 'be', 'at', 'least', 'as', 'to', 'the', 'habendum', 'during']\n",
        "\n",
        "[('the', 2576), ('good', 2555), ('to', 1861), ('of', 1846), ('and', 1794), ('a', 1253), ('i', 1102), ('in', 1010), ('as', 690), ('be', 674), ('that', 663), ('will', 543), ('have', 541), ('is', 529), ('for', 526), ('you', 496), ('it', 433), ('with', 412), ('your', 374), ('this', 319)]\n",
        "\n",
        "total positive words:  4849\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print adamsfd['good']\n",
      "\n",
      "#now, find the total number of words (not necessarily unique) and print them out\n",
      "adams_totalwc = 0\n",
      "for w in adamsfd:\n",
      "    adams_totalwc += adamsfd[w]\n",
      "print ''\n",
      "print 'Adams Total word count: ' + str(adams_totalwc)\n",
      "\n",
      "#now, find the total number of words (not necessarily unique) and print them out\n",
      "wash_totalwc = 0\n",
      "for w in washfd:\n",
      "    wash_totalwc += adamsfd[w]\n",
      "print ''\n",
      "print 'Wash Total word count: ' + str(wash_totalwc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2362\n",
        "\n",
        "Total word count: 813461\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Total word count: 752136\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now, we have to sort all (i.e. the most frequent 1000 non-stopw) words by the Pointwise Mutual Information metric\n",
      "#PMI = log(P(a^b)/(P(a)P(b))) = **log(p(a | b)/ p(a))**, where a is top-1000, b is \"good\".\n",
      "# ^^ above taken from https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
      "\n",
      "import math\n",
      "\n",
      "adams1000 = [wtup[0] for wtup in adamsfd.most_common(1000)]\n",
      "adamspmi = []\n",
      "\n",
      "# Assumption: P(w | good) = freq(w, \"good\") / freq(\"good\"). Below is freq(\"good\")\n",
      "fgood = float(adamsfd['good']) #should equal total number of 10-length strings identified around word \"good\".\n",
      "\n",
      "#for each word in top 1000, find the pmi score\n",
      "for w in adams1000:\n",
      "    # P(w | good) / P(w) = {freq(w, \"good\") / freq(\"good\")} / {freq(w)/adams_totalwc}\n",
      "    # pmi_nlog = (adamsposfd[w] / fgood) / (float(adamsfd[w]) / adams_totalwc) [SOME OF THIS STUFF JUST SCALES ALL]\n",
      "    pmi_nlog = float(adamsposfd[w])/adamsfd[w]\n",
      "    \n",
      "    if pmi_nlog < 0.01:\n",
      "        continue\n",
      "    pmi = math.log(pmi_nlog, 2)\n",
      "    adamspmi += [(w, pmi)]\n",
      "\n",
      "print adamspmi[:20]\n",
      "\n",
      "#sort words by pmi and print out the top ones\n",
      "adamspmi.sort(key = lambda tup: tup[1], reverse=True)\n",
      "print ''\n",
      "print adamspmi[:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', -5.242681343877626), ('would', -5.338959175415203), ('shall', -5.274133107455146), ('upon', -5.511616351151106), ('great', -5.099701491037551), ('one', -5.432764855992568), ('much', -4.785938248122432), ('every', -5.498073534230216), ('congress', -5.664647991379777), ('must', -5.131134406407755), ('letter', -6.024727085902377), ('time', -5.883729416160833), ('think', -4.955837913113843), ('make', -4.840572698951039), ('adams', -6.299901788557363), ('made', -5.665196560021013), ('two', -5.89539495677069), ('states', -6.409390936137703), ('us', -5.108524456778169), ('good', 0.11331432642782642)]\n",
        "\n",
        "[('good', 0.11331432642782642), ('deal', -1.8769891098946567), ('excellent', -3.205114429904613), ('wishes', -3.3846638502353255), ('policy', -3.394684437322677), ('officer', -3.425021587851466), ('health', -3.509861045480438), ('compliments', -3.548436624696042), ('commissions', -3.5924570372680806), ('effect', -3.599317793698226), ('capable', -3.6214883767462704), ('bad', -3.678071905112638), ('news', -3.8454900509443752), ('love', -3.8769891098946565), ('sense', -3.894290558828885), ('friendship', -3.9164766444377164), ('lead', -3.9354597478052895), ('disposition', -3.9740047914670558), ('friends', -4.013135402189134), ('acquainted', -4.02106161552783), ('knowledge', -4.028196891830652), ('man', -4.074928487506566), ('enjoy', -4.078951341394823), ('promote', -4.115477217419936), ('emperor', -4.124695747449315), ('judges', -4.129283016944967), ('respects', -4.139551352398794), ('fail', -4.160991876672305), ('reasons', -4.199155712766067), ('hope', -4.2048587915516995), ('secure', -4.205114429904612), ('worthy', -4.216317906926764), ('goes', -4.219168520462162), ('though', -4.247927513443586), ('warren', -4.247927513443586), ('length', -4.247927513443586), ('friend', -4.251989538766821), ('character', -4.258734268400168), ('sincerely', -4.28203536776385), ('says', -4.304854581528421), ('congratulate', -4.30875270613963), ('appears', -4.3110671022555955), ('care', -4.328781765928425), ('opportunity', -4.331843563752445), ('honest', -4.336283387864433), ('gives', -4.345774836841731), ('thank', -4.3513290113279215), ('trouble', -4.352516414720785), ('opinion', -4.35434957257974), ('possibly', -4.36923380966572)]\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#repeat above for Washington - create an inverted index, then find the best pmi words\n",
      "\n",
      "#Now, create an \"inverted index\" from Adams' files (except not really an iindex). Has the form:\n",
      "# (wordlist[i-5:i+5]), where i is the index of the word.\n",
      "\n",
      "#loop through all text files, and build inverted index.\n",
      "\n",
      "#start building with just one word? (\"good\" / \"excellent\")\n",
      "\n",
      "from nltk.probability import FreqDist\n",
      "\n",
      "#this is the proximity constant\n",
      "prox = 10\n",
      "posdata = []\n",
      "\n",
      "#find the full list of text files\n",
      "txtfiles = os.listdir(wash_text_path)\n",
      "\n",
      "#build a collection (posdata) of words that occur within 10 words of the word \"good\" in all files\n",
      "for fname in txtfiles:\n",
      "    f = open(wash_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    fwds = [w.lower() for w in fwds if w.isalpha()]\n",
      "    flen = len(fwds)\n",
      "    for wi in range(len(fwds)):\n",
      "        #if we find the word \"good\", store the whole set of words that are within distance \"prox\" of the word.\n",
      "        #we can add other words here later...\n",
      "        if fwds[wi] == \"good\":\n",
      "            posdata += fwds[max(0, wi-prox):min(flen, wi+prox)]\n",
      "    f.close()\n",
      "\n",
      "print posdata[:10]\n",
      "\n",
      "#now, create a freqdist from this collection\n",
      "washposfd = FreqDist(posdata)\n",
      "print ''\n",
      "print washposfd.most_common(20)\n",
      "print ''\n",
      "print 'total positive words: ',\n",
      "print len(washposfd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['me', 'very', 'but', 'at', 'the', 'same', 'time', 'acted', 'with', 'a']\n",
        "\n",
        "[('good', 1972), ('the', 1809), ('to', 1484), ('of', 1429), ('a', 1002), ('and', 919), ('i', 907), ('in', 756), ('as', 636), ('be', 572), ('you', 495), ('that', 478), ('for', 450), ('have', 443), ('is', 383), ('will', 348), ('it', 336), ('with', 325), ('your', 301), ('are', 272)]\n",
        "\n",
        "total positive words:  4844\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now, we have to sort all (i.e. the most frequent 1000 non-stopw) words by the Pointwise Mutual Information metric\n",
      "#PMI = log(P(a^b)/(P(a)P(b))) = **log(p(a | b)/ p(a))**, where a is top-1000, b is \"good\".\n",
      "# ^^ above taken from https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
      "\n",
      "import math\n",
      "\n",
      "wash1000 = [wtup[0] for wtup in washfd.most_common(1000)]\n",
      "washpmi = []\n",
      "\n",
      "# Assumption: P(w | good) = freq(w, \"good\") / freq(\"good\"). Below is freq(\"good\")\n",
      "fgood = float(washfd['good']) #should equal total number of 10-length strings identified around word \"good\".\n",
      "\n",
      "#for each word in top 1000, find the pmi score\n",
      "for w in wash1000:\n",
      "    # P(w | good) / P(w) = {freq(w, \"good\") / freq(\"good\")} / {freq(w)/adams_totalwc}\n",
      "    # pmi_nlog = (adamsposfd[w] / fgood) / (float(adamsfd[w]) / adams_totalwc) [SOME OF THIS STUFF JUST SCALES ALL]\n",
      "    pmi_nlog = float(washposfd[w])/washfd[w]\n",
      "    \n",
      "    if pmi_nlog < 0.01:\n",
      "        continue\n",
      "    pmi = math.log(pmi_nlog, 2)\n",
      "    washpmi += [(w, pmi)]\n",
      "\n",
      "print washpmi[:20]\n",
      "\n",
      "#sort words by pmi and print out the top ones\n",
      "washpmi.sort(key = lambda tup: tup[1], reverse=True)\n",
      "print ''\n",
      "print washpmi[:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', -5.185499026511515), ('mr', -5.963530513391959), ('shall', -5.094931512154189), ('upon', -5.344844451689377), ('sir', -6.119843321058662), ('one', -4.919340082442012), ('would', -5.150311443799337), ('men', -5.395177076918001), ('general', -6.116269689310716), ('time', -5.659401334399354), ('great', -5.261336689358781), ('much', -5.155674515836368), ('must', -5.362023500465161), ('washington', -6.136820267738638), ('two', -5.4269493075220065), ('every', -4.5942357269778995), ('think', -4.693120092896995), ('sent', -6.288358562193661), ('good', 0.08203187743627399), ('last', -6.105304154010526)]\n",
        "\n",
        "[('good', 0.08203187743627399), ('deal', -0.929610672108602), ('crop', -2.0552824355011894), ('wishes', -2.4521268180140643), ('behaviour', -2.6553518286125546), ('effect', -2.819427754358179), ('weather', -2.9827220085681647), ('health', -3.0), ('respects', -3.0752881273042374), ('pretty', -3.1043366598147353), ('heartily', -3.118941072723507), ('character', -3.1395513523987932), ('corn', -3.349408831309469), ('excuse', -3.4031241777353434), ('dare', -3.617383978413534), ('family', -3.7134370586699657), ('price', -3.7548875021634687), ('compliments', -3.7548875021634687), ('arms', -3.7895802203296283), ('sincere', -3.7914133781885826), ('beleive', -3.8073549220576046), ('happen', -3.830074998557688), ('oppertunity', -3.849665726915569), ('mind', -3.852442811586142), ('pair', -3.8609043903343387), ('tobacco', -3.863498000095138), ('hopes', -3.8838069824954773), ('nature', -3.886132035441721), ('provided', -3.897240425574799), ('bad', -3.949016071281195), ('look', -3.964896199044084), ('enough', -3.9732330904372146), ('future', -4.0), ('land', -4.017603818470919), ('equally', -4.044394119358453), ('tell', -4.056831222596676), ('easily', -4.058893689053568), ('peace', -4.075288127304238), ('well', -4.078002512001273), ('season', -4.078002512001273), ('afford', -4.08746284125034), ('merit', -4.08746284125034), ('water', -4.0945175987842894), ('ground', -4.095924419998536), ('friends', -4.096368394472232), ('promise', -4.108524456778169), ('live', -4.123382415505282), ('real', -4.139551352398794), ('support', -4.161887682376895), ('offer', -4.189824558880018)]\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now, print the results into a file\n",
      "\n",
      "fout = open('pmi_analysis', 'w')\n",
      "fout.write('PMI Analysis for Positive Words:\\n\\n')\n",
      "fout.write(\"+--------------------+-----------------------------+\\n\")\n",
      "fout.write(\"  Adams Word         |       Washington Word\\n\")\n",
      "fout.write(\"+--------------------+-----------------------------+\\n\")\n",
      "\n",
      "#start with positive information\n",
      "for i in range(200):\n",
      "    aword = adamspmi[i][0]\n",
      "    ascore = adamspmi[i][1]\n",
      "    wword = washpmi[i][0]\n",
      "    wscore = washpmi[i][1]\n",
      "    fout.write(str(i) + '. ' + aword + ': ' + \"%.3f\\t\" % ascore + '\\t' + wword + ': ' + \"%.3f\\n\" % wscore + '\\n')\n",
      "\n",
      "fout.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(adamspmi)\n",
      "print len(washpmi)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "825\n",
        "786\n"
       ]
      }
     ],
     "prompt_number": 32
    }
   ],
   "metadata": {}
  }
 ]
}