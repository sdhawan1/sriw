{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#In this file, track the way that topics change for Washington, Adams, and Jefferson overtime.\n",
      "#  can use either specific time periods that I decide are good, or time periods that are determined systematically.\n",
      "#     also: think about averaging over a year, or over several months.\n",
      "#  alternative: can also use topic modelling to figure this out.\n",
      "\n",
      "\n",
      "#two important time periods to start with for Adams: 1775 - 1777; 1777 - 1778. Over both of these time periods,\n",
      "#  Adams positivity and negativity rise and fall in a linear fashion. Surprisingly, they are positively \n",
      "#  (and not negatively) correlated.\n",
      "\n",
      "# This cell: for both of these time periods, Isolate all letters.\n",
      "\n",
      "#[usual setting] time_periods = [(1775, 1777), (1777, 1778)]\n",
      "#[for debugging]\n",
      "time_periods = [(1781, 1781)]\n",
      "\n",
      "# open up the adams-date file and isolate the letters with the right dates\n",
      "f = open('adams_fdates_out')\n",
      "dates = f.read().split('\\n')[:-1] #contains dates of all documents on file (not just \"writtenby\")\n",
      "years = [ int(d.split('/')[1]) for d in dates] #isolate years of letts.\n",
      "\n",
      "f_wby = open('adams_writtenby')\n",
      "writtenby = f_wby.read().split('\\n')[:-1] #contains indices of docs written by adams.\n",
      "wby_inds = [ int(w.split('_')[1]) for w in writtenby ]\n",
      "\n",
      "\n",
      "#now, isolate letters in the right time periods in the below list.\n",
      "tpinds = []\n",
      "for i in range(len(time_periods)):\n",
      "    tp = time_periods[i]\n",
      "    letters = [ i for i in wby_inds if ((years[i] >= tp[0]) and (years[i] <= tp[1])) ]\n",
      "    tpinds += [letters]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find out how many letters are in each time period\n",
      "#note: doesn't work well with list of length 1 as tpinds.\n",
      "print \"time period: 1775-7: \",\n",
      "print len(tpinds[0])\n",
      "print ''\n",
      "print \"time period: 1777-8: \",\n",
      "print len(tpinds[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "list index out of range",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-43-9d15394c2067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"time period: 1777-8: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpinds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mIndexError\u001b[0m: list index out of range"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "time period: 1775-7:  221\n",
        "\n",
        "time period: 1777-8: "
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now, perform analysis on the letters indicated in these time periods\n",
      "#start with a PMI analysis.\n",
      "#Recall that PMI = log(P(a^b)/(P(a)P(b))) = **log(p(a | b)/ p(a))**, where a is top-1000, b is \"good\".\n",
      "\n",
      "#this cell: compute the frequency with which every word is mentioned near the word \"good\". [WITH STEMMING]\n",
      "#  this will be very important to compute the final PMI score.\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import SnowballStemmer\n",
      "import os\n",
      "\n",
      "#an important constant when searching adams files\n",
      "adams_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')\n",
      "\n",
      "\n",
      "#this is the proximity constant\n",
      "prox = 10\n",
      "posdata = []\n",
      "\n",
      "#reconstruct the names of the full list of file indices.\n",
      "fnames = ['adams_' + str(i) for i in tpinds[0]]\n",
      "\n",
      "#build a collection (posdata) of words that occur within 10 words of the word \"good\" in all files\n",
      "#we also need a collection of all words that occur in this data set.\n",
      "allwds = []\n",
      "for fname in fnames:\n",
      "    f = open(adams_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    fwds = [w.lower() for w in fwds if w.isalpha()]\n",
      "    #this is in order to get a set of all words in these files (needed for pmi of this set of files)\n",
      "    allwds += fwds\n",
      "    flen = len(fwds)\n",
      "    for wi in range(len(fwds)):\n",
      "        #if we find the word \"excellent\", store the whole set of words that are within distance \"prox\" of the word.\n",
      "        #we can add other words here later...\n",
      "        # \"excellent\" may work a bit better than \"good\" - it is less commonly used in idioms (i.e. \"good lord!\")\n",
      "        if fwds[wi] == \"good\":\n",
      "            posdata += fwds[max(0, wi-prox):min(flen, wi+prox)]\n",
      "    f.close()\n",
      "\n",
      "print posdata[:10]\n",
      "\n",
      "####NEW: WE FIRST HAVE TO LEMMATIZE / STEM ALL WORDS (before the FD created)####\n",
      "stemmer = SnowballStemmer(\"english\")\n",
      "posdata = [stemmer.stem(w).encode('ascii', 'ignore') for w in posdata]\n",
      "alldata = [stemmer.stem(w).encode('ascii', 'ignore') for w in allwds]\n",
      "\n",
      "#now, create a freqdist from this collection. This freqdist contains \"freq(w ^ good)\", for all words w.\n",
      "adamsposfd = FreqDist(posdata)\n",
      "#next freqdist contains simple frequency of all words.\n",
      "adamsallfd = FreqDist(alldata)\n",
      "#eliminate stopwords\n",
      "sw = stopwords.words('english')\n",
      "sw = list(set( sw + [stemmer.stem(s) for s in sw] ))\n",
      "for w in sw:\n",
      "    if w in adamsposfd.keys():\n",
      "        adamsposfd[w] = 0\n",
      "    if w in adamsallfd.keys():\n",
      "        adamsallfd[w] = 0\n",
      "    \n",
      "print ''\n",
      "print adamsposfd.most_common(20)\n",
      "print ''\n",
      "print 'total positive words: ',\n",
      "print len(adamsposfd)\n",
      "\n",
      "#debug: print full fd into file [CHANGE FILENAME FOR STEMS.]\n",
      "f = open('debug/adams_posfd_stem', 'w')\n",
      "for item in adamsposfd.most_common(200):\n",
      "    f.write(item[0] + ' ' + str(item[1]) + '\\n')\n",
      "f.close()\n",
      "\n",
      "#NOTE: FOR THE YEAR 1775, THERE ARE TOO FEW DATA POINTS TO DO ANALYSIS: ONLY 39 OCCURRENCES OF 'GOOD',\n",
      "#  AND ONLY A FEW OCCURRENCES OF EACH OF THE TOP WORDS."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ['ariel', 'from', 'lorient', 'the', 'of', 'your', 'letter', 'was', 'found', 'in']\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('good', 99), ('great', 11), ('may', 9), ('upon', 9), ('congress', 8), ('shall', 7), ('would', 6), ('american', 6), ('think', 6), ('believ', 5), ('could', 5), ('noth', 5), ('wish', 5), ('communic', 5), ('send', 4), ('ever', 4), ('love', 4), ('two', 4), ('room', 4), ('give', 4)]\n",
        "\n",
        "total positive words:  522\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#debug: print out all words in \"allfd\"\n",
      "f = open('debug/adams_allfd_stem', 'w')\n",
      "for item in adamsallfd.most_common():\n",
      "    f.write(item[0] + ' ' + str(item[1]) + '\\n')\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "#compute the final metric: PMI = log(P(a^b)/(P(a)P(b))) = **log(p(w | good)/ p(w))**\n",
      "#  here, let P(w | good) = f(wd ^ good) / f(good). Also, p(w) = f(wd) / total_wc.\n",
      "#  therefore, PMI = log( [f(wd ^ good) / f(good)] * [total_wc / f(wd)] ).\n",
      "#  Since total_wc & f(good) are in every score, we can factor them out, giving:  PMI = log(f(wd^good)/f(wd)).\n",
      "\n",
      "adams_allsteps = {}\n",
      "adamspmi = []\n",
      "for w in adamsposfd:\n",
      "    #IMPORTANT: the below can be tried at 50 or 100; below that is not OK; at that is decent.\n",
      "    if adamsallfd[w] < 15:\n",
      "        continue\n",
      "    #PMI = log(f(wd^good) / f(wd))\n",
      "    pmi_nlog = float(adamsposfd[w]) / (adamsallfd[w]+15)\n",
      "    \n",
      "    if pmi_nlog < 0.0001:\n",
      "        continue\n",
      "    pmi = math.log(pmi_nlog, 2)\n",
      "    adamspmi += [(w, pmi)]\n",
      "    #FOR DEBUGGING\n",
      "    adams_allsteps[w] = [adamsposfd[w], adamsallfd[w], pmi_nlog, pmi]\n",
      "\n",
      "print adamspmi[:20]\n",
      "\n",
      "#sort words by pmi and print out the top ones\n",
      "adamspmi.sort(key = lambda tup: tup[1], reverse=True)\n",
      "print ''\n",
      "print adamspmi[:50]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('consider', -5.554588851677638), ('go', -6.794415866350106), ('privat', -5.169925001442312), ('depend', -4.2976805486406855), ('sorri', -5.169925001442312), ('young', -4.247927513443586), ('send', -4.078002512001273), ('present', -5.139551352398794), ('larg', -5.459431618637297), ('instruct', -5.491853096329675), ('everi', -4.826548487290915), ('minist', -5.643856189774724), ('far', -6.10852445677817), ('account', -4.523561956057013), ('govern', -5.169925001442312), ('affect', -4.0), ('hous', -6.643856189774724), ('small', -5.672425341971496), ('prevent', -5.426264754702098), ('fortun', -2.9634741239748865)]\n",
        "\n",
        "[('good', -0.20120709144581994), ('deal', -2.2094533656289497), ('fortun', -2.9634741239748865), ('fail', -3.1699250014423126), ('direct', -3.4594316186372978), ('rodney', -3.5545888516776376), ('agreabl', -3.6244908649077936), ('understand', -3.6244908649077936), ('sensibl', -3.6629650127224296), ('feel', -3.700439718141092), ('inde', -3.8073549220576046), ('effect', -3.8579809951275723), ('offic', -3.9307373375628862), ('languag', -3.938599455335857), ('take', -3.9448584458075393), ('recommend', -3.969626350956481), ('affect', -4.0), ('health', -4.0), ('compliment', -4.0), ('send', -4.078002512001273), ('old', -4.08746284125034), ('bound', -4.08746284125034), ('esteem', -4.08746284125034), ('mankind', -4.129283016944967), ('navig', -4.129283016944967), ('alway', -4.142957953842043), ('admir', -4.169925001442313), ('experi', -4.209453365628949), ('assembl', -4.209453365628949), ('end', -4.2223924213364485), ('accord', -4.2223924213364485), ('young', -4.247927513443586), ('boston', -4.247927513443586), ('assur', -4.273018494406416), ('talk', -4.285402218862249), ('depend', -4.2976805486406855), ('reason', -4.307428525192248), ('work', -4.321928094887363), ('truth', -4.321928094887363), ('wrote', -4.321928094887363), ('progress', -4.321928094887363), ('answer', -4.345774836841731), ('beg', -4.345774836841731), ('leav', -4.345774836841731), ('port', -4.357552004618084), ('necessari', -4.392317422778761), ('us', -4.405992359675837), ('abl', -4.409390936137702), ('hope', -4.417852514885898), ('find', -4.4329594072761065)]\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TEST RESULTS OF PREVIOUS CELL\n",
      "\n",
      "#debug stuff: Here are the parts of allsteps\n",
      "# adams_allsteps[w] = [adamsposfd[w], adamsallfd[w], pmi_nlog, pmi]\n",
      "\n",
      "#print adams_allsteps['fortunebecaus']\n",
      "#print adams_allsteps['american']\n",
      "print adams_allsteps['deal']\n",
      "\n",
      "#plot out the problem - will help you find best solution\n",
      "import matplotlib.pyplot as plt\n",
      "wordfreq = [adams_allsteps[a][1] for a in adams_allsteps]\n",
      "pmi = [adams_allsteps[a][3] for a in adams_allsteps]\n",
      "\n",
      "plt.plot(wordfreq, pmi, 'ro')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[8, 22, 0.21621621621621623, -2.2094533656289497]\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#next, print the above results into a file:\n",
      "#note: change the filename if the central pmi word is \"excellent\"!!\n",
      "filename = 'time_pds_pmi_stems/pmi_adams_1776_1776'\n",
      "f = open(filename, 'w')\n",
      "for w in adamspmi:\n",
      "    f.write(w[0] + ' ' + str(w[1]) + '\\n')\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Here, store the above as a function, that must be run multiple times. [NO STEMMING]\n",
      "# this function will find all the adams files within the two years, \"t0\" and \"t1\", inclusive.\n",
      "#   It will then perform a pmi ranking for all words in those time ranges, and print the results\n",
      "#   of this ranking into a file.\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "import math\n",
      "\n",
      "def pmi_rank_timerange (t0, t1):\n",
      "    \n",
      "    #-------------------------------------------------------------------------------------#\n",
      "    # Initial data processing: \n",
      "    #an important constant when searching adams files\n",
      "    adams_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')       \n",
      "    #this is the proximity constant\n",
      "    prox = 10\n",
      "    \n",
      "    \n",
      "    # open up the adams-date file and isolate the letters with the right dates\n",
      "    f = open('adams_fdates_out')\n",
      "    dates = f.read().split('\\n')[:-1] #contains dates of all documents on file (not just \"writtenby\")\n",
      "    years = [ int(d.split('/')[1]) for d in dates] #isolate years of letts.\n",
      "    \n",
      "    f_wby = open('adams_writtenby')\n",
      "    writtenby = f_wby.read().split('\\n')[:-1] #contains indices of docs written by adams.\n",
      "    wby_inds = [ int(w.split('_')[1]) for w in writtenby ]    \n",
      "    \n",
      "    #now, isolate letters in the right time periods in the below list.\n",
      "    tpinds = [ i for i in wby_inds if ((years[i] >= t0) and (years[i] <= t1)) ]\n",
      "    \n",
      "    #find out how many letters are in this time period\n",
      "    print \"\\n time period: \" + str(t0) + '-' + str(t1) + ': ' + str(len(tpinds))\n",
      "    \n",
      "    \n",
      "    #--------------------------------------------------------------------------------------#\n",
      "    # Compute the frequency with which every word is mentioned near the word \"good\":\n",
      "    #  this will be very important to compute the final PMI score.    \n",
      "     \n",
      "    #reconstruct the names of the full list of file indices.\n",
      "    fnames = ['adams_' + str(i) for i in tpinds]\n",
      "    \n",
      "    #build a collection (posdata) of words that occur within 10 words of the word \"good\" in all files\n",
      "    posdata = []\n",
      "    for fname in fnames:\n",
      "        f = open(adams_text_path + '/' + fname)\n",
      "        fwds = f.read().split()\n",
      "        fwds = [w.lower() for w in fwds if w.isalpha()]\n",
      "        flen = len(fwds)\n",
      "        for wi in range(len(fwds)):\n",
      "            #if we find the word \"excellent\", store the whole set of words that are within distance \"prox\" of the word.\n",
      "            #we can add other words here later...\n",
      "            # \"excellent\" may work a bit better than \"good\" - it is less commonly used in idioms (i.e. \"good lord!\")\n",
      "            if fwds[wi] == \"good\":\n",
      "                posdata += fwds[max(0, wi-prox):min(flen, wi+prox)]\n",
      "        f.close()\n",
      "        \n",
      "    #now, create a freqdist from this collection. This freqdist contains \"freq(w ^ good)\", for all words w.\n",
      "    adamsposfd = FreqDist(posdata)\n",
      "    #eliminate stopwords\n",
      "    for w in stopwords.words('english'):\n",
      "        if w in adamsposfd.keys():\n",
      "            adamsposfd[w] = 0\n",
      "       \n",
      "    \n",
      "    #----------------------------------------------------------------------------------------#\n",
      "    # Compute the final PMI for all words.\n",
      "    \n",
      "    #open the file with all of adams' words & store their freqs in a hashtable.\n",
      "    f = open('wf_analysis/adams_wds_nostop')\n",
      "    fwds = f.read().split('\\n')[:-1]\n",
      "    adamswf = {}\n",
      "    for l in fwds:\n",
      "        linearr = l.split(' ')\n",
      "        adamswf[linearr[0]] = int(linearr[1])\n",
      "    \n",
      "    #compute the final metric: PMI = log(P(a^b)/(P(a)P(b))) = **log(p(w | good)/ p(w))**\n",
      "    #  here, let P(w | good) = f(wd ^ good) / f(good). Also, p(w) = f(wd) / total_wc.\n",
      "    #  therefore, PMI = log( [f(wd ^ good) / f(good)] * [total_wc / f(wd)] ).\n",
      "    #  Since total_wc & f(good) are in every score, we can factor them out, giving:  PMI = log(f(wd^good)/f(wd)).\n",
      "    \n",
      "    adamspmi = []\n",
      "    for w in adamsposfd:\n",
      "        #PMI = log(f(wd^good) / f(wd))\n",
      "        pmi_nlog = float(adamsposfd[w]) / (adamswf[w]+3)\n",
      "        \n",
      "        if pmi_nlog < 0.0001:\n",
      "            continue\n",
      "        pmi = math.log(pmi_nlog, 2)\n",
      "        adamspmi += [(w, pmi)]\n",
      "    \n",
      "    #sort words by pmi and print out the top ones\n",
      "    adamspmi.sort(key = lambda tup: tup[1], reverse=True)\n",
      "    print ''\n",
      "    print adamspmi[:20]\n",
      "    \n",
      "    \n",
      "    #----------------------------------------------------------------------------------------#\n",
      "    # Print the above results into a file:\n",
      "    #If the central pmi word was \"excellent\"\n",
      "    #f = open('time_pds_pmi/pmi_excellent_adams_' + str(t0) + '_' + str(t1), 'w')\n",
      "    #If the central pmi word was \"good\"\n",
      "    f = open('time_pds_pmi/pmi_adams_' + str(t0) + '_' + str(t1), 'w')\n",
      "    for w in adamspmi:\n",
      "        f.write(w[0] + ' ' + str(w[1]) + '\\n')\n",
      "    f.close()\n",
      "\n",
      "\n",
      "# Run the above functions for *every year* in the time periods you want.\n",
      "\"\"\"\n",
      "daterange = range(1775, 1786)\n",
      "for i in daterange:\n",
      "    pmi_rank_timerange(i, i)\n",
      "\"\"\"\n",
      "\n",
      "#run the above function for the full range of Adams' letters.\n",
      "pmi_rank_timerange(1755, 1785)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " time period: 1755-1785: 1644\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('good', 5.1541576855839955), ('may', 4.0), ('would', 3.5849625007211565), ('upon', 3.3219280948873626), ('great', 3.192645077942396), ('shall', 3.1699250014423126), ('much', 2.584962500721156), ('one', 2.0), ('every', 1.5849625007211563), ('must', 1.3439544012173612), ('make', 0.9125371587496607), ('think', 0.9068905956085185), ('congress', 0.874469117916141), ('well', 0.1069152039165119), ('time', 0.0), ('give', -0.10309349296410361), ('letter', -0.10691520391651191), ('us', -0.13750352374993496), ('hope', -0.1979393776119089), ('could', -0.20645087746742632)]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#RUN THIS CODE AGAIN(?) ON 1780-1782\n",
      "\n",
      "#### Should be the same as the above, except that it works with stems, rather than words.\n",
      "\n",
      "# this function will find all the adams files within the two years, \"t0\" and \"t1\", inclusive.\n",
      "#   It will eliminate words with total frequency below \"minfreq\" in your dataset (reduces noise)\n",
      "#   It will then perform a pmi ranking for all words in those time ranges, and print the results\n",
      "#   of this ranking into a file.\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import SnowballStemmer\n",
      "import os\n",
      "import math\n",
      "\n",
      "def pmi_rank_timerange (t0, t1, minfreq):\n",
      "    \n",
      "    #-------------------------------------------------------------------------------------#\n",
      "    # Initial data processing: \n",
      "    #an important constant when searching adams files\n",
      "    adams_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')       \n",
      "    #this is the proximity constant\n",
      "    prox = 10\n",
      "    \n",
      "    \n",
      "    # open up the adams-date file and isolate the letters with the right dates\n",
      "    f = open('adams_fdates_out')\n",
      "    dates = f.read().split('\\n')[:-1] #contains dates of all documents on file (not just \"writtenby\")\n",
      "    years = [ int(d.split('/')[1]) for d in dates] #isolate years of letts.\n",
      "    \n",
      "    f_wby = open('adams_writtenby')\n",
      "    writtenby = f_wby.read().split('\\n')[:-1] #contains indices of docs written by adams.\n",
      "    wby_inds = [ int(w.split('_')[1]) for w in writtenby ]    \n",
      "    \n",
      "    #now, isolate letters in the right time periods in the below list.\n",
      "    tpinds = [ i for i in wby_inds if ((years[i] >= t0) and (years[i] <= t1)) ]\n",
      "    \n",
      "    #find out how many letters are in this time period\n",
      "    print \"\\n time period: \" + str(t0) + '-' + str(t1) + ': ' + str(len(tpinds))\n",
      "    \n",
      "    \n",
      "    #--------------------------------------------------------------------------------------#\n",
      "    # Compute the frequency with which every word is mentioned near the word \"good\":\n",
      "    #  this will be very important to compute the final PMI score.    \n",
      "     \n",
      "    #reconstruct the names of the full list of file indices.\n",
      "    fnames = ['adams_' + str(i) for i in tpinds]\n",
      "    \n",
      "    #build a collection (posdata) of words that occur within 10 words of the word \"good\" in all files\n",
      "    posdata = []\n",
      "    allwds = [] #need to collect all words occurring in this time period.\n",
      "    for fname in fnames:\n",
      "        f = open(adams_text_path + '/' + fname)\n",
      "        fwds = f.read().split()\n",
      "        fwds = [w.lower() for w in fwds if w.isalpha()]\n",
      "        #track all words occurring in all files:\n",
      "        allwds += fwds\n",
      "        flen = len(fwds)\n",
      "        for wi in range(len(fwds)):\n",
      "            #if we find the word \"excellent\", store the whole set of words that are within distance \"prox\" of the word.\n",
      "            #we can add other words here later...\n",
      "            # \"excellent\" may work a bit better than \"good\" - it is less commonly used in idioms (i.e. \"good lord!\")\n",
      "            if fwds[wi] == \"good\":\n",
      "                posdata += fwds[max(0, wi-prox):min(flen, wi+prox)]\n",
      "        f.close()\n",
      "        \n",
      "    ##NEW: WE FIRST HAVE TO LEMMATIZE / STEM ALL WORDS (before the FD created)##\n",
      "    stemmer = SnowballStemmer(\"english\")\n",
      "    posdata = [stemmer.stem(w).encode('ascii', 'ignore') for w in posdata]\n",
      "    alldata = [stemmer.stem(w).encode('ascii', 'ignore') for w in allwds]\n",
      "    \n",
      "    #now, create a freqdist from this collection. This freqdist contains \"freq(w ^ good)\", for all words w.\n",
      "    adamsposfd = FreqDist(posdata)\n",
      "    #this freqdist should contain a simple frequency of all words.\n",
      "    adamsallfd = FreqDist(alldata)\n",
      "    \n",
      "    #eliminate stopwords\n",
      "    sw = stopwords.words('english')\n",
      "    sw = list(set(sw + [stemmer.stem(s) for s in sw] ))\n",
      "    for w in sw:\n",
      "        if w in adamsposfd.keys():\n",
      "            adamsposfd[w] = 0\n",
      "        if w in adamsallfd.keys():\n",
      "            adamsallfd[w] = 0\n",
      "       \n",
      "    \n",
      "    #----------------------------------------------------------------------------------------#\n",
      "    # Compute the final PMI for all words.\n",
      "        \n",
      "    #  PMI = log(P(a^b)/(P(a)P(b))) = **log(p(w | good)/ p(w))**\n",
      "    #  here, let P(w | good) = f(wd ^ good) / f(good). Also, p(w) = f(wd) / total_wc.\n",
      "    #  therefore, PMI = log( [f(wd ^ good) / f(good)] * [total_wc / f(wd)] ).\n",
      "    #  Since total_wc & f(good) are in every score, we can factor them out, giving:  PMI = log(f(wd^good)/f(wd)).\n",
      "    \n",
      "    adams_allsteps = {}\n",
      "    adamspmi = []\n",
      "    for w in adamsposfd:\n",
      "        #eliminate words with below a certain user-defined frequency: otherwise words inflated due to low frequency.\n",
      "        if adamsallfd[w] < minfreq:\n",
      "            continue\n",
      "        \n",
      "        #PMI = log(f(wd^good) / f(wd))\n",
      "        pmi_nlog = float(adamsposfd[w]) / (adamsallfd[w]+minfreq)\n",
      "        \n",
      "        if pmi_nlog < 0.0001:\n",
      "            continue\n",
      "        pmi = math.log(pmi_nlog, 2)\n",
      "        adamspmi += [(w, pmi)]\n",
      "        #FOR DEBUGGING\n",
      "        adams_allsteps[w] = [adamsposfd[w], adamsallfd[w], pmi_nlog, pmi]\n",
      "    \n",
      "    #sort words by pmi and print out the top ones\n",
      "    adamspmi.sort(key = lambda tup: tup[1], reverse=True)\n",
      "    print ''\n",
      "    print adamspmi[:20]\n",
      "    #debug\n",
      "    #print ''\n",
      "    #print adams_allsteps['de']\n",
      "    \n",
      "    \n",
      "    #----------------------------------------------------------------------------------------#\n",
      "    # Print the above results into a file:\n",
      "    #If the central pmi word was \"excellent\"\n",
      "    #f = open('time_pds_pmi/pmi_excellent_adams_' + str(t0) + '_' + str(t1), 'w')\n",
      "    #If the central pmi word was \"good\"\n",
      "    f = open('time_pds_pmi_stems/pmi_adams_' + str(t0) + '_' + str(t1), 'w')\n",
      "    for w in adamspmi:\n",
      "        f.write(w[0] + ' ' + str(w[1]) + '\\n')\n",
      "    f.close()\n",
      "\n",
      "\n",
      "# Run the above functions for *every year* in the time periods you want.\n",
      "\n",
      "#daterange = range(1775, 1786)\n",
      "#for i in daterange:\n",
      "#    pmi_rank_timerange(i, i, 1)\n",
      "\n",
      "\n",
      "#run the above function for the full range of Adams' letters.\n",
      "pmi_rank_timerange(1755, 1785, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " time period: 1755-1785: 1644\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('good', -0.2321734421290334), ('deal', -2.8579809951275723), ('sens', -4.4329594072761065), ('believ', -4.451540833017832), ('preserv', -4.473931188332412), ('compliment', -4.473931188332412), ('reason', -4.493246331629542), ('term', -4.614709844115208), ('effect', -4.661065479806948), ('take', -4.700439718141093), ('offic', -4.71019868460342), ('friend', -4.73374169329192), ('affect', -4.749534267669262), ('hope', -4.772589503896928), ('find', -4.794415866350106), ('man', -4.802193216941825), ('servic', -4.81669278663694), ('charg', -4.826548487290915), ('opinion', -4.848622940429339), ('natur', -4.857980995127573)]\n"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Same function as above, except with a LEMMATIZER, instead of a stemmer\n",
      "# This should be used, because F likes Lemmatizers better than stemmers.\n",
      "\n",
      "# this function will find all the adams files within the two years, \"t0\" and \"t1\", inclusive.\n",
      "#   It will eliminate words with total frequency below \"minfreq\" in your dataset (reduces noise)\n",
      "#   It will then perform a pmi ranking for all words in those time ranges, and print the results\n",
      "#   of this ranking into a file.\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "import os\n",
      "import math\n",
      "\n",
      "def pmi_rank_timerange (t0, t1, minfreq):\n",
      "    \n",
      "    #-------------------------------------------------------------------------------------#\n",
      "    # Initial data processing: \n",
      "    #an important constant when searching adams files\n",
      "    adams_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')       \n",
      "    #this is the proximity constant\n",
      "    prox = 10\n",
      "    \n",
      "    \n",
      "    # open up the adams-date file and isolate the letters with the right dates\n",
      "    f = open('adams_fdates_out')\n",
      "    dates = f.read().split('\\n')[:-1] #contains dates of all documents on file (not just \"writtenby\")\n",
      "    years = [ int(d.split('/')[1]) for d in dates] #isolate years of letts.\n",
      "    \n",
      "    f_wby = open('adams_writtenby')\n",
      "    writtenby = f_wby.read().split('\\n')[:-1] #contains indices of docs written by adams.\n",
      "    wby_inds = [ int(w.split('_')[1]) for w in writtenby ]    \n",
      "    \n",
      "    #now, isolate letters in the right time periods in the below list.\n",
      "    tpinds = [ i for i in wby_inds if ((years[i] >= t0) and (years[i] <= t1)) ]\n",
      "    \n",
      "    #find out how many letters are in this time period\n",
      "    print \"\\n time period: \" + str(t0) + '-' + str(t1) + ': ' + str(len(tpinds))\n",
      "    \n",
      "    \n",
      "    #--------------------------------------------------------------------------------------#\n",
      "    # Compute the frequency with which every word is mentioned near the word \"good\":\n",
      "    #  this will be very important to compute the final PMI score.    \n",
      "     \n",
      "    #reconstruct the names of the full list of file indices.\n",
      "    fnames = ['adams_' + str(i) for i in tpinds]\n",
      "    \n",
      "    #build a collection (posdata) of words that occur within 10 words of the word \"good\" in all files\n",
      "    posdata = []\n",
      "    allwds = [] #need to collect all words occurring in this time period.\n",
      "    for fname in fnames:\n",
      "        f = open(adams_text_path + '/' + fname)\n",
      "        fwds = f.read().split()\n",
      "        fwds = [w.lower() for w in fwds if w.isalpha()]\n",
      "        #track all words occurring in all files:\n",
      "        allwds += fwds\n",
      "        flen = len(fwds)\n",
      "        for wi in range(len(fwds)):\n",
      "            #if we find the word \"excellent\", store the whole set of words that are within distance \"prox\" of the word.\n",
      "            #we can add other words here later...\n",
      "            # \"excellent\" may work a bit better than \"good\" - it is less commonly used in idioms (i.e. \"good lord!\")\n",
      "            if fwds[wi] == \"good\":\n",
      "                posdata += fwds[max(0, wi-prox):min(flen, wi+prox)]\n",
      "        f.close()\n",
      "        \n",
      "    ##NEW: WE FIRST HAVE TO LEMMATIZE / STEM ALL WORDS (before the FD created)##\n",
      "    wnl = WordNetLemmatizer()\n",
      "    posdata = [wnl.lemmatize(w.lower()).encode('ascii', 'ignore') for w in posdata]\n",
      "    alldata = [wnl.lemmatize(w.lower()).encode('ascii', 'ignore') for w in allwds]\n",
      "    \n",
      "    #now, create a freqdist from this collection. This freqdist contains \"freq(w ^ good)\", for all words w.\n",
      "    adamsposfd = FreqDist(posdata)\n",
      "    #this freqdist should contain a simple frequency of all words.\n",
      "    adamsallfd = FreqDist(alldata)\n",
      "    \n",
      "    #eliminate stopwords\n",
      "    sw = stopwords.words('english')\n",
      "    sw = list(set(sw + [wnl.lemmatize(s) for s in sw] ))\n",
      "    for w in sw:\n",
      "        if w in adamsposfd.keys():\n",
      "            adamsposfd[w] = 0\n",
      "        if w in adamsallfd.keys():\n",
      "            adamsallfd[w] = 0\n",
      "       \n",
      "    \n",
      "    #----------------------------------------------------------------------------------------#\n",
      "    # Compute the final PMI for all words.\n",
      "        \n",
      "    #  PMI = log(P(a^b)/(P(a)P(b))) = **log(p(w | good)/ p(w))**\n",
      "    #  here, let P(w | good) = f(wd ^ good) / f(good). Also, p(w) = f(wd) / total_wc.\n",
      "    #  therefore, PMI = log( [f(wd ^ good) / f(good)] * [total_wc / f(wd)] ).\n",
      "    #  Since total_wc & f(good) are in every score, we can factor them out, giving:  PMI = log(f(wd^good)/f(wd)).\n",
      "    \n",
      "    adams_allsteps = {}\n",
      "    adamspmi = []\n",
      "    for w in adamsposfd:\n",
      "        #eliminate words with below a certain user-defined frequency: otherwise words inflated due to low frequency.\n",
      "        if adamsallfd[w] < minfreq:\n",
      "            continue\n",
      "        \n",
      "        #PMI = log(f(wd^good) / f(wd))\n",
      "        pmi_nlog = float(adamsposfd[w]) / (adamsallfd[w]+minfreq)\n",
      "        \n",
      "        if pmi_nlog < 0.0001:\n",
      "            continue\n",
      "        pmi = math.log(pmi_nlog, 2)\n",
      "        adamspmi += [(w, pmi)]\n",
      "        #FOR DEBUGGING\n",
      "        adams_allsteps[w] = [adamsposfd[w], adamsallfd[w], pmi_nlog, pmi]\n",
      "    \n",
      "    #sort words by pmi and print out the top ones\n",
      "    adamspmi.sort(key = lambda tup: tup[1], reverse=True)\n",
      "    print ''\n",
      "    print adamspmi[:20]\n",
      "    #debug\n",
      "    #print ''\n",
      "    #print adams_allsteps['de']\n",
      "    \n",
      "    \n",
      "    #----------------------------------------------------------------------------------------#\n",
      "    # Print the above results into a file:\n",
      "    #If the central pmi word was \"excellent\"\n",
      "    #f = open('time_pds_pmi/pmi_excellent_adams_' + str(t0) + '_' + str(t1), 'w')\n",
      "    #If the central pmi word was \"good\"\n",
      "    f = open('time_pds_pmi_lemmas/pmi_adams_' + str(t0) + '_' + str(t1), 'w')\n",
      "    for w in adamspmi:\n",
      "        f.write(w[0] + ' ' + str(w[1]) + '\\n')\n",
      "    f.close()\n",
      "\n",
      "\n",
      "# Run the above functions for *every year* in the time periods you want.\n",
      "\n",
      "#daterange = range(1780, 1782)\n",
      "#for i in daterange:\n",
      "#    pmi_rank_timerange(i, i, 10)\n",
      "\n",
      "\n",
      "#run the above function for the full range of Adams' letters.\n",
      "pmi_rank_timerange(1783, 1783, 15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " time period: 1783-1783: 176\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('good', -0.06763871687536319), ('possible', -2.9259994185562235), ('friend', -3.2394659346953896), ('manner', -3.321928094887362), ('nature', -3.3692338096657193), ('sorry', -3.415037499278844), ('son', -3.415037499278844), ('believe', -3.502500340529183), ('inform', -3.5443205162238103), ('much', -3.6472467789754988), ('reason', -3.700439718141092), ('care', -3.700439718141092), ('whose', -3.8073549220576046), ('people', -3.8073549220576046), ('desire', -3.9068905956085187), ('six', -4.0), ('allow', -4.0), ('change', -4.044394119358453), ('idea', -4.044394119358453), ('finish', -4.044394119358453)]\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for Adams, plot the number of letters he has written over the significant years of his life.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      " \n",
      "daterange = range(1775, 1786)\n",
      "#number of letters written in each time period (i.e. year).\n",
      "timepds = [91, 110, 65, 89, 105, 374, 221, 210, 176, 127, 18]\n",
      "\n",
      "#this creates a scatterplot of the above data\n",
      "plt.plot(daterange, timepds, 'ro')\n",
      "plt.xlabel('Year of letter written')\n",
      "plt.ylabel('Number of letters')\n",
      "plt.title('Scatterplot of Letter Frequencies')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#the below will create a histogram\n",
      "\"\"\"\n",
      "letter_dates = []\n",
      "for i in range(len(timepds)):\n",
      "    letter_dates += timepds[i]*[daterange[i]]\n",
      "\n",
      "plt.hist(letter_dates)\n",
      "plt.xlabel('Year of letter written')\n",
      "plt.ylabel('Number of letters')\n",
      "plt.title('Histogram of Letter Frequencies')\n",
      "plt.show()\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "\"\\nletter_dates = []\\nfor i in range(len(timepds)):\\n    letter_dates += timepds[i]*[daterange[i]]\\n\\nplt.hist(letter_dates)\\nplt.xlabel('Year of letter written')\\nplt.ylabel('Number of letters')\\nplt.title('Histogram of Letter Frequencies')\\nplt.show()\\n\""
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#correlation between number of letters written and sentiment\n",
      "\n",
      "#This, I think, shows the powerful impact of a large set of neutral letters (letters with no sentiment).\n",
      "#  perhaps it will be useful to make histograms of each year (or to filter out-low sentiment letters.)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#number of letters written in each time period (i.e. year).\n",
      "#  [outlier at (18 letters, 14 sentiment) excluded.]\n",
      "timepds = [91, 110, 65, 89, 105, 374, 221, 210, 176, 127]\n",
      "senti_level = [10.1, 12.6, 9.9, 10.7, 10.6, 9.0, 9.8, 9.5, 9.9, 10.5]\n",
      "\n",
      "#create a scatterplot of the above data\n",
      "plt.plot(timepds, senti_level, 'ro')\n",
      "plt.xlabel('Number of letters written (per time pd.)')\n",
      "plt.ylabel('Avg. Sentiment level of letters.')\n",
      "plt.title('Scatterplot of letter volume vs. sentiment')\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now, find difference between normal pmi and pmi for a given year.\n",
      "\n",
      "\n",
      "pmi_file_location = 'time_pds_pmi/pmi_adams_'\n",
      "\n",
      "f = open(pmi_file_location + '1755_1785')\n",
      "ftxt = f.read().split('\\n')[:-1]\n",
      "total_pmi={}\n",
      "for l in ftxt:\n",
      "    total_pmi[ l.split(' ')[0] ] = float(l.split(' ')[1])\n",
      "f.close()\n",
      "\n",
      "##[DEBUG: should be range(1775, 1786)]##\n",
      "daterange = range(1775, 1786)\n",
      "#END DEBUG#\n",
      "\n",
      "year_pmi = {}\n",
      "pmi_diff = []\n",
      "for y in daterange:\n",
      "    f = open(pmi_file_location + str(y) + '_' + str(y))\n",
      "    ftxt = []\n",
      "    ftxt = f.read().split('\\n')[:-1]\n",
      "    fpmis = []\n",
      "    fpmis = [(l.split(' ')[0], float(l.split(' ')[1])) for l in ftxt]\n",
      "    f.close()\n",
      "    \n",
      "    #store the pmi for every word in the year.\n",
      "    year_pmi[y] = {}\n",
      "    for p in fpmis:\n",
      "        year_pmi[y][p[0]] = p[1]\n",
      "    \n",
      "    min_pmi = min(year_pmi[y].values())\n",
      "    #find the difference between overall pmi and pmi in year y.\n",
      "    pmi_diff = [] #need this to be refreshed.\n",
      "    for w in total_pmi.keys():\n",
      "        if w in year_pmi[y].keys():\n",
      "            #find relative difference in pmi. If this is a bad metric, can try z-scores as well.\n",
      "            wpmi = year_pmi[y][w]            \n",
      "        else:\n",
      "            wpmi = min_pmi\n",
      "        \n",
      "        #want a positive change to be stored as a positive diff - may need to doublecheck this.\n",
      "        if (total_pmi[w] != 0.0):\n",
      "            pmi_diff += [(w, (wpmi - total_pmi[w]) / abs(total_pmi[w])) ]\n",
      "        \n",
      "    #sort the pmi diffs (descending)\n",
      "    # this means we will see words that are used more often come up first.\n",
      "    pmi_diff.sort(key = lambda tup: tup[1], reverse = True)\n",
      "    #print pmi_diff\n",
      "    \n",
      "   \n",
      "    f = open(pmi_file_location + 'diff_' + str(y), 'w')\n",
      "    for t in pmi_diff:\n",
      "        f.write(t[0] + ' ' + str(t[1]) + '\\n')\n",
      "    f.close()         \n",
      "    \n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}