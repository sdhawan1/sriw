{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find the most frequent words by Washington\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "\n",
      "wash_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/washington_text')\n",
      "txtfiles = os.listdir(wash_text_path)\n",
      "\n",
      "washfd = FreqDist()\n",
      "\n",
      "#loop through all collected text files, and put all words in FreqDist\n",
      "for fname in txtfiles:\n",
      "    f = open(wash_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    for w in fwds:\n",
      "        if w.isalpha():\n",
      "            washfd.update([w.lower()])\n",
      "    f.close()\n",
      "\n",
      "#finally, remove all stopwords from the FreqDist (english and french)\n",
      "for w in stopwords.words('english'):\n",
      "    if w in washfd.keys():\n",
      "        washfd[w] = 0\n",
      "        \n",
      "#for w in stopwords.words('french'):\n",
      "#    if w in washfd.keys():\n",
      "#        washfd[w] = 0\n",
      "\n",
      "#print out the most frequent common\n",
      "print washfd.most_common(50)\n",
      "\n",
      "#print out the total number of unique words:\n",
      "print \"\"\n",
      "print \"Number of unique words: \",\n",
      "print len(washfd)\n",
      "\n",
      "#find the total number of words used by Washington.\n",
      "total_wds = 0\n",
      "for w in washfd:\n",
      "    total_wds += washfd[w]\n",
      "print ''\n",
      "print 'Total word count: ' + str(total_wds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', 5495), ('mr', 5117), ('shall', 4067), ('upon', 3617), ('sir', 3199), ('one', 3026), ('would', 2557), ('men', 2525), ('general', 2428), ('time', 2426), ('great', 2378), ('much', 2317), ('must', 2262), ('washington', 2111), ('two', 2108), ('every', 2029), ('think', 1966), ('sent', 1954), ('ditto', 1867), ('good', 1863), ('last', 1790), ('made', 1776), ('letter', 1742), ('officers', 1724), ('make', 1700), ('give', 1674), ('yr', 1674), ('send', 1671), ('take', 1662), ('hope', 1567), ('fort', 1551), ('new', 1510), ('servt', 1493), ('without', 1432), ('best', 1431), ('order', 1430), ('john', 1395), ('well', 1368), ('know', 1358), ('woud', 1304), ('pr', 1299), ('soon', 1294), ('first', 1286), ('pay', 1274), ('part', 1257), ('many', 1221), ('also', 1205), ('orders', 1186), ('day', 1186), ('could', 1178)]\n",
        "39607\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#In this cell, extract names of French letters written by John Adams. Need to be eliminated from analysis.\n",
      "### Note: these french letters are taken from a file, not by scanning through all of JA's letters ###\n",
      "\n",
      "f = open('adams_french_letters', 'r')\n",
      "skip = True\n",
      "frenchfnames = []\n",
      "for line in f:\n",
      "    #skips the first line in the document\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    #each additional line is the name of a file.\n",
      "    frenchfnames += [line[:-1]]\n",
      "f.close()\n",
      "\n",
      "#check that \"frenchfnames\" has the correct length (should be 705); print out the first few.\n",
      "print \"number of french letters found: \",\n",
      "print len(frenchfnames)\n",
      "print frenchfnames[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of french letters found:  705\n",
        "['adams_1192', 'adams_1352', 'adams_3575', 'adams_3942', 'adams_1732', 'adams_3678', 'adams_3909', 'adams_1500', 'adams_4700', 'adams_3556']\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### IN NLTK 3.0, \"FREQDIST.INC\" WAS REPLACED WITH \"FREQDIST.UPDATE\" ###\n",
      "### IN THE NEW NLTK, \"FD.ITEMS()\" DOESN'T RETURN SORTED LIST - FOR THAT, USE \"FD.MOST_COMMON(N)\" ###\n",
      "\n",
      "#This cell: Find words most frequently used by Adams\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "\n",
      "adams_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')\n",
      "txtfiles = os.listdir(adams_text_path)\n",
      "\n",
      "adamsfd = FreqDist()\n",
      "\n",
      "##### TAKE OUT ALL FRENCH FILES #### [need above cell to run...]\n",
      "for fname in txtfiles:\n",
      "    if fname in frenchfnames:\n",
      "        txtfiles.remove(fname)\n",
      "\n",
      "#loop through all collected text files, and put all words in FreqDist\n",
      "for fname in txtfiles:\n",
      "    f = open(adams_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    for w in fwds:\n",
      "        if w.isalpha():\n",
      "            adamsfd.update([w.lower()])\n",
      "    f.close()\n",
      "\n",
      "#finally, remove all stopwords from the FreqDist (english and french)\n",
      "for w in stopwords.words('english'):\n",
      "    if w in adamsfd.keys():\n",
      "        adamsfd[w] = 0\n",
      "        \n",
      "for w in stopwords.words('french'):\n",
      "    if w in adamsfd.keys():\n",
      "        adamsfd[w] = 0\n",
      "\n",
      "# [print the fifty first entries in \"most common\". Note: this is a sorted list]\n",
      "print adamsfd.most_common(50)\n",
      "# print total unique words used.\n",
      "print ''\n",
      "print 'Total number of unique words used: ',\n",
      "print len(adamsfd)\n",
      "\n",
      "#now, find the total number of words (not necessarily unique) and print them out\n",
      "total_wds = 0\n",
      "for w in adamsfd:\n",
      "    total_wds += adamsfd[w]\n",
      "print ''\n",
      "print 'Total word count: ' + str(total_wds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', 6588), ('would', 6476), ('shall', 5611), ('upon', 5292), ('great', 4972), ('one', 4449), ('sir', 3991), ('much', 3945), ('every', 3254), ('congress', 3145), ('must', 3119), ('letter', 3060), ('time', 2657), ('think', 2638), ('make', 2636), ('adams', 2600), ('made', 2588), ('two', 2500), ('states', 2465), ('us', 2415), ('good', 2362), ('general', 2352), ('without', 2350), ('well', 2349), ('united', 2284), ('new', 2273), ('give', 2205), ('could', 2186), ('said', 2134), ('know', 2098), ('last', 2064), ('america', 2028), ('many', 2028), ('take', 2013), ('first', 1913), ('american', 1905), ('hope', 1881), ('john', 1861), ('never', 1807), ('people', 1799), ('dear', 1781), ('yet', 1764), ('present', 1762), ('part', 1726), ('see', 1701), ('might', 1688), ('wish', 1630), ('letters', 1603), ('humble', 1600), ('cannot', 1550)]\n",
        "\n",
        "Total number of unique words used:  40066\n",
        "\n",
        "Total word count: 813461\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Now, find the total word count of all these letters (needs to be for A and W)\n",
      "total_wds = 0\n",
      "for w in washfd:\n",
      "    total_wds += adamsfd[w]\n",
      "print ''\n",
      "print 'Total word count: ' + str(total_wds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Total word count: 752136\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Custom sentimentality metric: take the most common 1000 words used by Adams, then weight their sentimentality\n",
      "#  by their frequency. Then find the top 200 of the resulting list.\n",
      "\n",
      "### Strangely, after experimenting more with log-frequency, Adams' and W's words are very similar ###\n",
      "### Experiment more with frequency and log-frequency\n",
      "\n",
      "###### [ATTN]: THIS METRIC HAS BEEN MODIFIED AND CURRENTLY REFLECTS ONLY SENTIWORDNET - MAY WANT TO CHANGE!!!!\n",
      "### ONE GOOD IDEA IS TO LET THE POSITIVITY BE THE SENTIWORDNET SCORE + THE PMI!!!\n",
      "    \n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "import math\n",
      "\n",
      "#find the words that Adams uses most frequently and possibly change their format.\n",
      "adams_1000 = adamsfd.most_common(1000)\n",
      "\n",
      "#multiply the frequency of these words by their sentiment index from sentiwordnet.\n",
      "adams_pos = []\n",
      "adams_neg = []\n",
      "\n",
      "for wf in adams_1000:\n",
      "    word = wf[0]\n",
      "    freq = wf[1]\n",
      "    logfreq = math.log(freq, 2)\n",
      "    #first, determine if this word exists in Sentiwordnet\n",
      "    ssets = swn.senti_synsets(word)\n",
      "    if len(ssets) == 0:\n",
      "        #doesn't deal with misspelled words: may want to correct minor misspellings later.\n",
      "        continue\n",
      "        \n",
      "    #for each word in sentiwordnet and \"adams_1000\", find its average positivity and negativity.\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for s in ssets:\n",
      "        pos += s.pos_score()\n",
      "        neg += s.neg_score()\n",
      "    \n",
      "    #the average we are taking doesn't take into account the frequency with which each type of word\n",
      "    #   is used. This may be a next step.\n",
      "    ###### [ATTN]: THIS METRIC HAS BEEN MODIFIED AND CURRENTLY REFLECTS ONLY SENTIWORDNET - MAY WANT TO CHANGE!!!!\n",
      "    if pos > 0.01:\n",
      "        #posweight = logfreq * pos / len(ssets)\n",
      "        posweight = pos / len(ssets)\n",
      "        adams_pos += [(word, posweight)]\n",
      "    if neg > 0.01:\n",
      "        #negweight = logfreq * neg / len(ssets)\n",
      "        negweight = neg / len(ssets)\n",
      "        adams_neg += [(word, negweight)]\n",
      "\n",
      "        \n",
      "#now, we should have two lists of words that have a positive or negative connotation in sentiwordnet,\n",
      "#   plus their weighted scores. The next step is to sort these words by score and print results.\n",
      "adams_pos.sort(key = lambda tup: tup[1], reverse=True)\n",
      "adams_neg.sort(key = lambda tup: tup[1], reverse=True)\n",
      "\n",
      "#print out the best words\n",
      "print adams_pos[:20]\n",
      "print \"\"\n",
      "print adams_neg[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('happy', 0.875), ('proper', 0.75), ('honourable', 0.75), ('worthy', 0.6875), ('sincere', 0.625), ('virtue', 0.625), ('affection', 0.625), ('important', 0.5833333333333334), ('honorable', 0.5625), ('happiness', 0.5625), ('good', 0.515625), ('better', 0.5154444444444445), ('opportunity', 0.5), ('goods', 0.5), ('greater', 0.5), ('importance', 0.5), ('shew', 0.5), ('wholly', 0.5), ('duke', 0.5), ('absolutely', 0.5)]\n",
        "\n",
        "[('sufficient', 0.75), ('dangerous', 0.75), ('difficult', 0.6875), ('sorry', 0.625), ('ill', 0.625), ('glorious', 0.625), ('danger', 0.5625), ('bad', 0.525), ('might', 0.5), ('humble', 0.5), ('fear', 0.5), ('notwithstanding', 0.5), ('mightinesses', 0.5), ('afraid', 0.5), ('never', 0.4375), ('sentiments', 0.4375), ('little', 0.425), ('neutral', 0.4166666666666667), ('les', 0.375), ('impossible', 0.375)]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now, write Adams' most sentimental words into a file.\n",
      "import sys\n",
      "print adams_pos[:10]\n",
      "print adams_neg[:10]\n",
      "\n",
      "\n",
      "f = open('adams_senti_wds', 'w')\n",
      "f.write('Positive:\\n')\n",
      "for i in range(200):\n",
      "    word = adams_pos[i][0]\n",
      "    score = adams_pos[i][1]\n",
      "    f.write(str(i) + '. ' + word + '\\t' + \"%.3f\\n\" % score)\n",
      "    \n",
      "f.write('=====================\\nNegative:\\n')\n",
      "for i in range(len(adams_neg)):\n",
      "    word = adams_neg[i][0]\n",
      "    score = adams_neg[i][1]\n",
      "    f.write(str(i) + '. ' + word + '\\t' + '%.3f\\n' % score)\n",
      "    \n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('happy', 0.875), ('proper', 0.75), ('honourable', 0.75), ('worthy', 0.6875), ('sincere', 0.625), ('virtue', 0.625), ('affection', 0.625), ('important', 0.5833333333333334), ('honorable', 0.5625), ('happiness', 0.5625)]\n",
        "[('sufficient', 0.75), ('dangerous', 0.75), ('difficult', 0.6875), ('sorry', 0.625), ('ill', 0.625), ('glorious', 0.625), ('danger', 0.5625), ('bad', 0.525), ('might', 0.5), ('humble', 0.5)]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Custom sentimentality metric: take the most common 1000 words used by Washington, then weight their sentimentality\n",
      "#  by their frequency. Then find the top 200 of the resulting list.\n",
      "\n",
      "### MAY WANT TO CHANGE THIS TO LOG-FREQUENCY ###\n",
      "### AFTER TESTING THIS, I HAVE FOUND THAT LOG-FREQUENCY IS MUCH BETTER- IT'S A LOT LESS NOISY!!!!\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "import math\n",
      "\n",
      "#find the words that Adams uses most frequently and possibly change their format.\n",
      "wash_1000 = washfd.most_common(1000)\n",
      "\n",
      "#multiply the frequency of these words by their sentiment index from sentiwordnet.\n",
      "wash_pos = []\n",
      "wash_neg = []\n",
      "\n",
      "for wf in wash_1000:\n",
      "    word = wf[0]\n",
      "    freq = wf[1]\n",
      "    logfreq = math.log(freq, 2)\n",
      "    #first, determine if this word exists in Sentiwordnet\n",
      "    ssets = swn.senti_synsets(word)\n",
      "    if len(ssets) == 0:\n",
      "        #doesn't deal with misspelled words: may want to correct minor misspellings later.\n",
      "        continue\n",
      "        \n",
      "    #for each word in sentiwordnet and \"adams_1000\", find its average positivity and negativity.\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for s in ssets:\n",
      "        pos += s.pos_score()\n",
      "        neg += s.neg_score()\n",
      "    \n",
      "    #the average we are taking doesn't take into account the frequency with which each type of word\n",
      "    #   is used. This may be a next step.\n",
      "    if pos > 0.01:\n",
      "        posweight = freq * pos / len(ssets)\n",
      "        wash_pos += [(word, posweight)]\n",
      "    ### [TEMPORARY: DEBUGGING (see diff. between log-freq and freq): CHANGE THIS!] ###\n",
      "    #  normal: should be \"neg\" > 0.01, then negweight = logfreq * neg / len(ssets)\n",
      "    if neg > 0.01:\n",
      "        negweight = logfreq * neg / len(ssets)\n",
      "        wash_neg += [(word, negweight)]\n",
      "\n",
      "        \n",
      "#now, we should have two lists of words that have a positive or negative connotation in sentiwordnet,\n",
      "#   plus their weighted scores. The next step is to sort these words by score and print results.\n",
      "wash_pos.sort(key = lambda tup: tup[1], reverse=True)\n",
      "wash_neg.sort(key = lambda tup: tup[1], reverse=True)\n",
      "\n",
      "#print out the best words\n",
      "print wash_pos[:20]\n",
      "print \"\"\n",
      "print wash_neg[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('good', 960.609375), ('proper', 756.75), ('best', 593.6783478260869), ('well', 447.4728), ('might', 376.125), ('much', 372.375), ('new', 314.5833333333333), ('necessary', 311.25), ('hope', 304.69444444444446), ('better', 299.4732222222222), ('dear', 284.0), ('must', 282.75), ('think', 263.30357142857144), ('respect', 241.66666666666666), ('know', 240.47916666666666), ('happy', 226.625), ('agreeable', 210.0), ('time', 202.16666666666666), ('goods', 186.5), ('esteem', 186.3)]\n",
        "\n",
        "[('sufficient', 6.6458151794965445), ('unhappy', 5.6875), ('sorry', 5.19271189383713), ('might', 4.985052945306091), ('humble', 4.819217956995236), ('ill', 4.5984700028863035), ('guilty', 4.493640349300011), ('little', 4.293339660153423), ('bad', 4.26244406317984), ('fear', 4.13339327034745), ('never', 4.130380260829756), ('notwithstanding', 3.7173141138183627), ('sentiments', 3.3809651988714), ('mutilated', 3.3534429395022145), ('must', 3.2501534374136973), ('trouble', 3.0776452266440657), ('glad', 2.876428482577876), ('impossible', 2.8329708193791143), ('militia', 2.7642156409201175), ('different', 2.7369447546021015)]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Print out the most sentimental words found for Washington\n",
      "\n",
      "#now, write Washington's most sentimental words into a file.\n",
      "f = open('wash_senti_wds', 'w')\n",
      "f.write('Positive:\\n')\n",
      "for i in range(200):\n",
      "    word = wash_pos[i][0]\n",
      "    score = wash_pos[i][1]\n",
      "    f.write(str(i) + '. ' + word + '\\t' + \"%.3f\\n\" % score)\n",
      "    \n",
      "f.write('=====================\\nNegative:\\n')\n",
      "for i in range(200):\n",
      "    word = wash_neg[i][0]\n",
      "    score = wash_neg[i][1]\n",
      "    f.write(str(i) + '. ' + word + '\\t' + '%.3f\\n' % score)\n",
      "    \n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#condense the above two files into a single file\n",
      "\n",
      "#extract info from files created above first\n",
      "f = open('adams_senti_wds')\n",
      "ftxt = f.read()\n",
      "[fpos, fneg] = ftxt.split('=====================\\n')\n",
      "lpos = fpos.split('\\n')\n",
      "lneg = fneg.split('\\n')\n",
      "\n",
      "#populate list of positive tuples\n",
      "adams_poswds = []\n",
      "skip = True\n",
      "for l in lpos:\n",
      "    #skip the first line, which just explains things.\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    larr = l.split()\n",
      "    #if the line is empty, skip.\n",
      "    if len(larr) == 0:\n",
      "        continue\n",
      "    word = larr[1]\n",
      "    score = float(larr[2])\n",
      "    adams_poswds += [(word, score)]\n",
      "    \n",
      "#populate list of negative tuples\n",
      "adams_negwds = []\n",
      "skip = True\n",
      "for l in lneg:\n",
      "    #skip the first line, which just explains things.\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    larr = l.split()\n",
      "    #if the line is empty, skip.\n",
      "    if len(larr) == 0:\n",
      "        continue\n",
      "    word = larr[1]\n",
      "    score = float(larr[2])\n",
      "    adams_negwds += [(word, score)]\n",
      "f.close()\n",
      "    \n",
      "#now extract info from washington's files\n",
      "f = open('wash_senti_wds')\n",
      "ftxt = f.read()\n",
      "[fpos, fneg] = ftxt.split('=====================\\n')\n",
      "lpos = fpos.split('\\n')\n",
      "lneg = fneg.split('\\n')\n",
      "\n",
      "#populate list of positive tuples\n",
      "wash_poswds = []\n",
      "skip = True\n",
      "for l in lpos:\n",
      "    #skip the first line, which just explains things.\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    larr = l.split()\n",
      "    #if the line is empty, skip.\n",
      "    if len(larr) == 0:\n",
      "        continue\n",
      "    word = larr[1]\n",
      "    score = float(larr[2])\n",
      "    wash_poswds += [(word, score)]\n",
      "    \n",
      "#populate list of negative tuples\n",
      "wash_negwds = []\n",
      "skip = True\n",
      "for l in lneg:\n",
      "    #skip the first line, which just explains things.\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    larr = l.split()\n",
      "    #if the line is empty, skip.\n",
      "    if len(larr) == 0:\n",
      "        continue\n",
      "    word = larr[1]\n",
      "    score = float(larr[2])\n",
      "    wash_negwds += [(word, score)]\n",
      "f.close()\n",
      "\n",
      "#now, write all the above information into a single file.\n",
      "fout = open('senti_wordfreq_analysis', 'w')\n",
      "fout.write('Analysis for Positive Words:\\n\\n')\n",
      "fout.write(\"+--------------------+-----------------------------+\\n\")\n",
      "fout.write(\"  Adams Word         |       Washington Word\\n\")\n",
      "fout.write(\"+--------------------+-----------------------------+\\n\")\n",
      "\n",
      "#start with positive information\n",
      "for i in range(200):\n",
      "    aword = adams_poswds[i][0]\n",
      "    ascore = adams_poswds[i][1]\n",
      "    wword = wash_poswds[i][0]\n",
      "    wscore = wash_poswds[i][1]\n",
      "    fout.write(str(i) + '. ' + aword + ': ' + \"%.3f\\t\" % ascore + '\\t' + wword + ': ' + \"%.3f\\n\" % wscore + '\\n')\n",
      "\n",
      "#repeat above for all the negative words\n",
      "fout.write('\\n\\n Analysis for Negative Words:\\n\\n')\n",
      "fout.write(\"+--------------------+-----------------------------+\\n\")\n",
      "fout.write(\"  Adams Word         |       Washington Word\\n\")\n",
      "fout.write(\"+--------------------+-----------------------------+\\n\")    \n",
      "for i in range(len(adams_negwds)):\n",
      "    aword = adams_negwds[i][0]\n",
      "    ascore = adams_negwds[i][1]\n",
      "    wword = wash_negwds[i][0]\n",
      "    wscore = wash_negwds[i][1]\n",
      "    fout.write(str(i) + '. ' + aword + ': ' + \"%.3f\\t\" % ascore + '\\t' + wword + ': ' + \"%.3f\\n\" % wscore + '\\n')\n",
      "\n",
      "\n",
      "\n",
      "fout.close()\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}