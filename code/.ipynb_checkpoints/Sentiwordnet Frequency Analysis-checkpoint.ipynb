{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find the most frequent words by Washington\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "\n",
      "wash_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/washington_text')\n",
      "txtfiles = os.listdir(wash_text_path)\n",
      "\n",
      "washfd = FreqDist()\n",
      "\n",
      "#loop through all collected text files, and put all words in FreqDist\n",
      "for fname in txtfiles:\n",
      "    f = open(wash_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    for w in fwds:\n",
      "        if w.isalpha():\n",
      "            washfd.inc(w.lower())\n",
      "    f.close()\n",
      "\n",
      "#finally, remove all stopwords from the FreqDist (english and french)\n",
      "for w in stopwords.words('english'):\n",
      "    if w in washfd.keys():\n",
      "        washfd[w] = 0\n",
      "        \n",
      "#for w in stopwords.words('french'):\n",
      "#    if w in washfd.keys():\n",
      "#        washfd[w] = 0\n",
      "\n",
      "print washfd.items()[:50]\n",
      "#go through all the files and create a big dictionary of words.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', 5495), ('mr', 5117), ('shall', 4067), ('upon', 3617), ('sir', 3199), ('one', 3026), ('would', 2557), ('men', 2525), ('general', 2428), ('time', 2426), ('great', 2378), ('much', 2317), ('must', 2262), ('washington', 2111), ('two', 2108), ('every', 2029), ('think', 1966), ('sent', 1954), ('ditto', 1867), ('good', 1863), ('last', 1790), ('made', 1776), ('letter', 1742), ('officers', 1724), ('make', 1700), ('give', 1674), ('yr', 1674), ('send', 1671), ('take', 1662), ('hope', 1567), ('fort', 1551), ('new', 1510), ('servt', 1493), ('without', 1432), ('best', 1431), ('order', 1430), ('john', 1395), ('well', 1368), ('know', 1358), ('woud', 1304), ('pr', 1299), ('soon', 1294), ('first', 1286), ('pay', 1274), ('part', 1257), ('many', 1221), ('also', 1205), ('day', 1186), ('orders', 1186), ('could', 1178)]\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#In this cell, extract names of French letters written by John Adams. Need to be eliminated from analysis.\n",
      "\n",
      "f = open('adams_french_letters', 'r')\n",
      "skip = True\n",
      "frenchfnames = []\n",
      "for line in f:\n",
      "    #skips the first line in the document\n",
      "    if skip:\n",
      "        skip = False\n",
      "        continue\n",
      "    #each additional line is the name of a file.\n",
      "    frenchfnames += [line[:-1]]\n",
      "f.close()\n",
      "\n",
      "#check that \"frenchfnames\" has the correct length (should be 705); print out the first few.\n",
      "print \"number of french letters found: \",\n",
      "print len(frenchfnames)\n",
      "print frenchfnames[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of french letters found:  705\n",
        "['adams_1192', 'adams_1352', 'adams_3575', 'adams_3942', 'adams_1732', 'adams_3678', 'adams_3909', 'adams_1500', 'adams_4700', 'adams_3556']\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Find words most frequently used by Adams\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.probability import FreqDist\n",
      "import os\n",
      "\n",
      "adams_text_path = os.path.abspath(os.getcwd() + '/../data/crawling/adams_text')\n",
      "txtfiles = os.listdir(adams_text_path)\n",
      "\n",
      "adamsfd = FreqDist()\n",
      "\n",
      "##### TAKE OUT ALL FRENCH FILES #### [need above cell to run...]\n",
      "for fname in txtfiles:\n",
      "    if fname in frenchfnames:\n",
      "        txtfiles.remove(fname)\n",
      "\n",
      "#loop through all collected text files, and put all words in FreqDist\n",
      "for fname in txtfiles:\n",
      "    f = open(adams_text_path + '/' + fname)\n",
      "    fwds = f.read().split()\n",
      "    for w in fwds:\n",
      "        if w.isalpha():\n",
      "            adamsfd.inc(w.lower())\n",
      "    f.close()\n",
      "\n",
      "#finally, remove all stopwords from the FreqDist (english and french)\n",
      "for w in stopwords.words('english'):\n",
      "    if w in adamsfd.keys():\n",
      "        adamsfd[w] = 0\n",
      "        \n",
      "for w in stopwords.words('french'):\n",
      "    if w in adamsfd.keys():\n",
      "        adamsfd[w] = 0\n",
      "\n",
      "print adamsfd.items()[:50]\n",
      "#go through all the files and create a big dictionary of words."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', 6588), ('would', 6476), ('shall', 5611), ('upon', 5292), ('great', 4972), ('one', 4449), ('sir', 3991), ('much', 3945), ('every', 3254), ('congress', 3145), ('must', 3119), ('letter', 3060), ('time', 2657), ('think', 2638), ('make', 2636), ('adams', 2600), ('made', 2588), ('two', 2500), ('states', 2465), ('us', 2415), ('good', 2362), ('general', 2352), ('without', 2350), ('well', 2349), ('united', 2284), ('new', 2273), ('give', 2205), ('could', 2186), ('said', 2134), ('know', 2098), ('last', 2064), ('america', 2028), ('many', 2028), ('take', 2013), ('first', 1913), ('american', 1905), ('hope', 1881), ('john', 1861), ('never', 1807), ('people', 1799), ('dear', 1781), ('yet', 1764), ('present', 1762), ('part', 1726), ('see', 1701), ('might', 1688), ('wish', 1630), ('letters', 1603), ('humble', 1600), ('cannot', 1550)]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#doublecheck what is in adamsfd.\n",
      "print adamsfd.items()[:4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('may', 6588), ('would', 6476), ('shall', 5611), ('upon', 5292)]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#figure out how to extract sentiment information from sentiwordnet\n",
      "import nltk\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "\n",
      "#the main problem is that words can be used in many different senses. We have to figure out what sense the word\n",
      "#  is used in, and then from that, we can infer the sentimentality of the word.\n",
      "#  For now, it may be good to just take an average of all the senses."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Custom sentimentality metric: take the most common 1000 words used by Adams, then weight their sentimentality\n",
      "#  by their frequency. Then find the top 200 of the resulting list.\n",
      "import nltk\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "\n",
      "#find the words that Adams uses most frequently and possibly change their format.\n",
      "adams_1000 = adamsfd.items()[:1000]\n",
      "\n",
      "#multiply the frequency of these words by their sentiment index from sentiwordnet.\n",
      "adams_pos = []\n",
      "adams_neg = []\n",
      "\n",
      "for wf in adams_1000:\n",
      "    word = wf[0]\n",
      "    freq = wf[1]\n",
      "    #first, determine if this word exists in Sentiwordnet\n",
      "    ssets = swn.senti_synsets(word)\n",
      "    if len(ssets) == 0:\n",
      "        #doesn't deal with misspelled words: may want to correct minor misspellings later.\n",
      "        continue\n",
      "        \n",
      "    #for each word in sentiwordnet and \"adams_1000\", find its average positivity and negativity.\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for s in ssets:\n",
      "        pos += s.pos_score()\n",
      "        neg += s.neg_score()\n",
      "    \n",
      "    #the average we are taking doesn't take into account the frequency with which each type of word\n",
      "    #   is used. This may be a next step.\n",
      "    if pos > 0.01:\n",
      "        posweight = freq * pos / len(ssets)\n",
      "        adams_pos += [(word, posweight)]\n",
      "    if neg > 0.01:\n",
      "        negweight = freq * neg / len(ssets)\n",
      "        adams_neg += [(word, negweight)]\n",
      "\n",
      "        \n",
      "#now, we should have two lists of words that have a positive or negative connotation in sentiwordnet,\n",
      "#   plus their weighted scores. The next step is to sort these words by score and print results.\n",
      "adams_pos.sort(key = lambda tup: tup[1], reverse=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "cannot import name sentiwordnet",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-0059dcd7042d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#  by their frequency. Then find the top 200 of the resulting list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentiwordnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mswn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#find the words that Adams uses most frequently and possibly change their format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: cannot import name sentiwordnet"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}